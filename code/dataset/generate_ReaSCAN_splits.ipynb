{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrips for generating splits\n",
    "This script assums you have the main ReaSCAN generated by the generate_ReaSCAN.py script. After that, you can use this file to generate/extrapolate different splits. In the future, we may consolidate two files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, OrderedDict\n",
    "import os\n",
    "from typing import List\n",
    "from typing import Tuple\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def isnotebook():\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter\n",
    "if isnotebook():\n",
    "    device = torch.device(\"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "FORMAT = \"%(asctime)-15s %(message)s\"\n",
    "logging.basicConfig(format=FORMAT, level=logging.INFO,\n",
    "                    datefmt=\"%Y-%m-%d %H:%M\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from world import *\n",
    "from object_vocabulary import *\n",
    "from vocabulary import *\n",
    "from grammer import *\n",
    "from simulator import *\n",
    "from relation_graph import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P1: gSCAN Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_path_to_data = \"../../ReaSCAN-v1.0/ReaSCAN-compositional-p1/data-train.txt\"\n",
    "logger.info(f\"Reading dataset from file: {p1_path_to_data}...\")\n",
    "p1_data_json = json.load(open(p1_path_to_data, \"r\"))\n",
    "\n",
    "p1_all_fake_train = p1_data_json[\"examples\"][\"train\"]\n",
    "# for dev and test, it is simple, let us just shuffle, and random select.\n",
    "len(p1_all_fake_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For generating the splits, we actually have to go through compositional splits first\n",
    "# and then consider random splits like dev and test. Because, we don't want things mixed up\n",
    "# in the dev and test. Dev and test should only contain commands that appear in the train,\n",
    "# so a total random partition at the end should work.\n",
    "\n",
    "# We do the splits step-by-step!\n",
    "p1_id_example_map = OrderedDict({})\n",
    "p1_id_splits_map = OrderedDict({})\n",
    "index = 0\n",
    "for example in p1_data_json[\"examples\"][\"train\"]:\n",
    "    p1_id_example_map[index] = example\n",
    "    p1_id_splits_map[index] = set([]) # set of splits that this example belongs to.\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_splits_distribution = OrderedDict({})\n",
    "p1_splits_assignment = OrderedDict({})\n",
    "for index, splits in p1_id_splits_map.items():\n",
    "    if len(splits) == 0:\n",
    "        split = \"train\" # let us split this up later!\n",
    "        if split in p1_splits_distribution.keys():\n",
    "            p1_splits_distribution[split] += 1\n",
    "        else:\n",
    "            p1_splits_distribution[split] = 1\n",
    "        \n",
    "        if split in p1_splits_assignment:\n",
    "            p1_splits_assignment[split].append(index)\n",
    "        else:\n",
    "            p1_splits_assignment[split] = [index]\n",
    "    else:   \n",
    "        assert False\n",
    "\n",
    "# Let us further segment train into dev and test!\n",
    "gscan_dev_size = int(len(p1_all_fake_train)*0.01)\n",
    "gscan_test_size = int(len(p1_all_fake_train)*0.052)\n",
    "p1_all_example_id = p1_splits_assignment[\"train\"]\n",
    "random.shuffle(p1_all_example_id)\n",
    "p1_train_example_id = p1_all_example_id[:(-gscan_dev_size-gscan_test_size)]\n",
    "p1_dev_example_id = p1_all_example_id[(-gscan_dev_size-gscan_test_size):-gscan_dev_size]\n",
    "p1_test_example_id = p1_all_example_id[-gscan_dev_size:]\n",
    "p1_splits_assignment[\"train\"] = p1_train_example_id\n",
    "p1_splits_assignment[\"dev\"] = p1_dev_example_id\n",
    "p1_splits_assignment[\"test\"] = p1_test_example_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, all_ids in p1_splits_assignment.items():\n",
    "    print(f\"for {split} split, we have {len(all_ids)} examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remake our data file accordingly.\n",
    "updated_examples = OrderedDict({})\n",
    "for split, all_ids in p1_splits_assignment.items():\n",
    "    updated_examples[split] = []\n",
    "    for _id in all_ids:\n",
    "        updated_examples[split].append(p1_id_example_map[_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it to the disk\n",
    "p1_data_json[\"examples\"] = updated_examples\n",
    "with open(\"../../data-files-updated/ReaSCAN-compositional-p1/data-compositional-splits.txt\", \"w\") as fd:\n",
    "    json.dump(p1_data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P2: Single Clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2_path_to_data = \"../../ReaSCAN-v1.0/ReaSCAN-compositional-p2/data-train.txt\"\n",
    "logger.info(f\"Reading dataset from file: {p2_path_to_data}...\")\n",
    "p2_data_json = json.load(open(p2_path_to_data, \"r\"))\n",
    "\n",
    "p2_all_fake_train = p2_data_json[\"examples\"][\"train\"]\n",
    "# for dev and test, it is simple, let us just shuffle, and random select.\n",
    "len(p2_all_fake_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For generating the splits, we actually have to go through compositional splits first\n",
    "# and then consider random splits like dev and test. Because, we don't want things mixed up\n",
    "# in the dev and test. Dev and test should only contain commands that appear in the train,\n",
    "# so a total random partition at the end should work.\n",
    "\n",
    "# We do the splits step-by-step!\n",
    "p2_id_example_map = OrderedDict({})\n",
    "p2_id_splits_map = OrderedDict({})\n",
    "index = 0\n",
    "for example in p2_data_json[\"examples\"][\"train\"]:\n",
    "    p2_id_example_map[index] = example\n",
    "    p2_id_splits_map[index] = set([]) # set of splits that this example belongs to.\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2_splits_distribution = OrderedDict({})\n",
    "p2_splits_assignment = OrderedDict({})\n",
    "for index, splits in p2_id_splits_map.items():\n",
    "    if len(splits) == 0:\n",
    "        split = \"train\" # let us split this up later!\n",
    "        if split in p2_splits_distribution.keys():\n",
    "            p2_splits_distribution[split] += 1\n",
    "        else:\n",
    "            p2_splits_distribution[split] = 1\n",
    "        \n",
    "        if split in p2_splits_assignment:\n",
    "            p2_splits_assignment[split].append(index)\n",
    "        else:\n",
    "            p2_splits_assignment[split] = [index]\n",
    "    else:   \n",
    "        assert False\n",
    "\n",
    "# Let us further segment train into dev and test!\n",
    "gscan_dev_size = int(len(p2_all_fake_train)*0.01)\n",
    "gscan_test_size = int(len(p2_all_fake_train)*0.052)\n",
    "p2_all_example_id = p2_splits_assignment[\"train\"]\n",
    "random.shuffle(p2_all_example_id)\n",
    "p2_train_example_id = p2_all_example_id[:(-gscan_dev_size-gscan_test_size)]\n",
    "p2_dev_example_id = p2_all_example_id[(-gscan_dev_size-gscan_test_size):-gscan_dev_size]\n",
    "p2_test_example_id = p2_all_example_id[-gscan_dev_size:]\n",
    "p2_splits_assignment[\"train\"] = p2_train_example_id\n",
    "p2_splits_assignment[\"dev\"] = p2_dev_example_id\n",
    "p2_splits_assignment[\"test\"] = p2_test_example_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, all_ids in p2_splits_assignment.items():\n",
    "    print(f\"for {split} split, we have {len(all_ids)} examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remake our data file accordingly.\n",
    "updated_examples = OrderedDict({})\n",
    "for split, all_ids in p2_splits_assignment.items():\n",
    "    updated_examples[split] = []\n",
    "    for _id in all_ids:\n",
    "        updated_examples[split].append(p2_id_example_map[_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it to the disk\n",
    "p2_data_json[\"examples\"] = updated_examples\n",
    "with open(\"../../data-files-updated/ReaSCAN-compositional-p2/data-compositional-splits.txt\", \"w\") as fd:\n",
    "    json.dump(p2_data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P3: Double Clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3_path_to_data = \"../../ReaSCAN-v1.0/ReaSCAN-compositional-p3/data-train.txt\"\n",
    "logger.info(f\"Reading dataset from file: {p3_path_to_data}...\")\n",
    "p3_data_json = json.load(open(p3_path_to_data, \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3_all_fake_train = p3_data_json[\"examples\"][\"train\"]\n",
    "# for dev and test, it is simple, let us just shuffle, and random select.\n",
    "len(p3_all_fake_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For generating the splits, we actually have to go through compositional splits first\n",
    "# and then consider random splits like dev and test. Because, we don't want things mixed up\n",
    "# in the dev and test. Dev and test should only contain commands that appear in the train,\n",
    "# so a total random partition at the end should work.\n",
    "\n",
    "# We do the splits step-by-step!\n",
    "p3_id_example_map = OrderedDict({})\n",
    "p3_id_splits_map = OrderedDict({})\n",
    "index = 0\n",
    "for example in p3_data_json[\"examples\"][\"train\"]:\n",
    "    p3_id_example_map[index] = example\n",
    "    p3_id_splits_map[index] = set([]) # set of splits that this example belongs to.\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3_splits_distribution = OrderedDict({})\n",
    "p3_splits_assignment = OrderedDict({})\n",
    "for index, splits in p3_id_splits_map.items():\n",
    "    if len(splits) == 0:\n",
    "        split = \"train\" # let us split this up later!\n",
    "        if split in p3_splits_distribution.keys():\n",
    "            p3_splits_distribution[split] += 1\n",
    "        else:\n",
    "            p3_splits_distribution[split] = 1\n",
    "        \n",
    "        if split in p3_splits_assignment:\n",
    "            p3_splits_assignment[split].append(index)\n",
    "        else:\n",
    "            p3_splits_assignment[split] = [index]\n",
    "    else:   \n",
    "        assert False\n",
    "\n",
    "# Let us further segment train into dev and test!\n",
    "gscan_dev_size = int(len(p3_all_fake_train)*0.01)\n",
    "gscan_test_size = int(len(p3_all_fake_train)*0.052)\n",
    "p3_all_example_id = p3_splits_assignment[\"train\"]\n",
    "random.shuffle(p3_all_example_id)\n",
    "p3_train_example_id = p3_all_example_id[:(-gscan_dev_size-gscan_test_size)]\n",
    "p3_dev_example_id = p3_all_example_id[(-gscan_dev_size-gscan_test_size):-gscan_dev_size]\n",
    "p3_test_example_id = p3_all_example_id[-gscan_dev_size:]\n",
    "p3_splits_assignment[\"train\"] = p3_train_example_id\n",
    "p3_splits_assignment[\"dev\"] = p3_dev_example_id\n",
    "p3_splits_assignment[\"test\"] = p3_test_example_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, all_ids in p3_splits_assignment.items():\n",
    "    print(f\"for {split} split, we have {len(all_ids)} examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remake our data file accordingly.\n",
    "updated_examples = OrderedDict({})\n",
    "for split, all_ids in p3_splits_assignment.items():\n",
    "    updated_examples[split] = []\n",
    "    for _id in all_ids:\n",
    "        updated_examples[split].append(p3_id_example_map[_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it to the disk\n",
    "p3_data_json[\"examples\"] = updated_examples\n",
    "with open(\"../../data-files-updated/ReaSCAN-compositional-p3/data-compositional-splits.txt\", \"w\") as fd:\n",
    "    json.dump(p3_data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P3: Double Clause (combing with P3 sharding, this section is shared across all patterns as well!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "pattern = \"p4\"\n",
    "special_condition = \"\"\n",
    "if special_condition != \"\":\n",
    "    prefix = f\"{pattern}-{special_condition}\"\n",
    "else:\n",
    "    prefix = pattern\n",
    "sharding_dir = f\"../../data-files-{prefix}/\"\n",
    "if pattern == \"p3\":\n",
    "    upper_limit = 3375\n",
    "elif pattern == \"p2\":\n",
    "    upper_limit = 2025\n",
    "elif pattern == \"p1\":\n",
    "    upper_limit = 675\n",
    "elif pattern == \"p4\":\n",
    "    upper_limit = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_command = set([])\n",
    "for subdir, dirs, files in os.walk(sharding_dir):\n",
    "    if \"jobid\" in subdir:\n",
    "        \n",
    "        # Completeness check!\n",
    "        logging_file = os.path.join(subdir, \"generator.log\")\n",
    "        with open(logging_file) as f:\n",
    "            content = f.readlines()\n",
    "        # you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "        content = [x.strip() for x in content]\n",
    "        completed = False\n",
    "        for c in content:\n",
    "            if \"==FINISH==\" in c:\n",
    "                completed = True\n",
    "                break\n",
    "        jobid = logging_file.split(\"/\")[-2].split(\"-\")[-1]\n",
    "        print(f\"jobid={jobid}, status=complete={completed}\")\n",
    "        if not completed:\n",
    "            break\n",
    "        \n",
    "        # Uniqueness check!\n",
    "        data_file_path = os.path.join(subdir, \"data-train.txt\")\n",
    "        print(f\"scanning for file: {data_file_path}\")\n",
    "        data_file = json.load(open(data_file_path, \"r\"))\n",
    "        for example in data_file[\"examples\"][\"train\"]:\n",
    "            command_split = re.split(',a,|,the,', example['command'])\n",
    "            command_mono = \",\".join(command_split)\n",
    "            unique_command.add(command_mono)\n",
    "assert len(unique_command) > upper_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_example_combined = {}\n",
    "per_command_mono_count = {}\n",
    "for subdir, dirs, files in os.walk(sharding_dir):\n",
    "    if \"jobid\" in subdir:\n",
    "        data_file_path = os.path.join(subdir, \"data-train.txt\")\n",
    "        print(f\"Collecting for file: {data_file_path}\")\n",
    "        data_file = json.load(open(data_file_path, \"r\"))\n",
    "        for example in data_file[\"examples\"][\"train\"]:\n",
    "            command_split = re.split(',a,|,the,', example['command'])\n",
    "            command_mono = \",\".join(command_split)\n",
    "            if command_mono in per_command_mono_count.keys():\n",
    "                if per_command_mono_count[command_mono] == 180: # for p4 this may never hit!\n",
    "                    continue # we are not adding this example since redundant!\n",
    "                per_command_mono_count[command_mono] += 1\n",
    "            else:\n",
    "                per_command_mono_count[command_mono] = 1\n",
    "            if command_mono in shared_example_combined.keys():\n",
    "                shared_example_combined[command_mono].append(example)\n",
    "            else:\n",
    "                shared_example_combined[command_mono] = [example]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to disk!\n",
    "import random\n",
    "shared_examples = []\n",
    "commands_mono = list(shared_example_combined.keys())\n",
    "random.shuffle(commands_mono)\n",
    "for i in range(upper_limit):\n",
    "    examples_to_include = shared_example_combined[commands_mono[i]]\n",
    "    for example in examples_to_include:\n",
    "        shared_examples.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file[\"examples\"][\"train\"] = shared_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(shared_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../../data-files-{prefix}/ReaSCAN-compositional-{prefix}/data-train.txt\", \"w\") as fd:\n",
    "    json.dump(data_file, fd, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P3-RD: Double Clause with Only Random Distractors (and some contextual distractors, which are also random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3_rd_path_to_data = \"../../data-files-updated/ReaSCAN-compositional-p3-rd/data-train.txt\"\n",
    "logger.info(f\"Reading dataset from file: {p3_rd_path_to_data}...\")\n",
    "p3_rd_data_json = json.load(open(p3_rd_path_to_data, \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3_rd_all_fake_train = p3_rd_data_json[\"examples\"][\"train\"]\n",
    "# for dev and test, it is simple, let us just shuffle, and random select.\n",
    "len(p3_rd_all_fake_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For generating the splits, we actually have to go through compositional splits first\n",
    "# and then consider random splits like dev and test. Because, we don't want things mixed up\n",
    "# in the dev and test. Dev and test should only contain commands that appear in the train,\n",
    "# so a total random partition at the end should work.\n",
    "\n",
    "# We do the splits step-by-step!\n",
    "p3_rd_id_example_map = OrderedDict({})\n",
    "p3_rd_id_splits_map = OrderedDict({})\n",
    "index = 0\n",
    "for example in p3_rd_data_json[\"examples\"][\"train\"]:\n",
    "    p3_rd_id_example_map[index] = example\n",
    "    p3_rd_id_splits_map[index] = set([]) # set of splits that this example belongs to.\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3_rd_splits_distribution = OrderedDict({})\n",
    "p3_rd_splits_assignment = OrderedDict({})\n",
    "for index, splits in p3_rd_id_splits_map.items():\n",
    "    if len(splits) == 0:\n",
    "        split = \"train\" # let us split this up later!\n",
    "        if split in p3_rd_splits_distribution.keys():\n",
    "            p3_rd_splits_distribution[split] += 1\n",
    "        else:\n",
    "            p3_rd_splits_distribution[split] = 1\n",
    "        \n",
    "        if split in p3_rd_splits_assignment:\n",
    "            p3_rd_splits_assignment[split].append(index)\n",
    "        else:\n",
    "            p3_rd_splits_assignment[split] = [index]\n",
    "    else:   \n",
    "        assert False\n",
    "\n",
    "# Let us further segment train into dev and test!\n",
    "gscan_dev_size = int(len(p3_rd_all_fake_train)*0.01)\n",
    "gscan_test_size = int(len(p3_rd_all_fake_train)*0.052)\n",
    "p3_rd_all_example_id = p3_rd_splits_assignment[\"train\"]\n",
    "random.shuffle(p3_rd_all_example_id)\n",
    "p3_rd_train_example_id = p3_rd_all_example_id[:(-gscan_dev_size-gscan_test_size)]\n",
    "p3_rd_dev_example_id = p3_rd_all_example_id[(-gscan_dev_size-gscan_test_size):-gscan_dev_size]\n",
    "p3_rd_test_example_id = p3_rd_all_example_id[-gscan_dev_size:]\n",
    "p3_rd_splits_assignment[\"train\"] = p3_rd_train_example_id\n",
    "p3_rd_splits_assignment[\"dev\"] = p3_rd_dev_example_id\n",
    "p3_rd_splits_assignment[\"test\"] = p3_rd_test_example_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, all_ids in p3_rd_splits_assignment.items():\n",
    "    print(f\"for {split} split, we have {len(all_ids)} examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remake our data file accordingly.\n",
    "updated_examples = OrderedDict({})\n",
    "for split, all_ids in p3_rd_splits_assignment.items():\n",
    "    updated_examples[split] = []\n",
    "    for _id in all_ids:\n",
    "        updated_examples[split].append(p3_rd_id_example_map[_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it to the disk\n",
    "p3_rd_data_json[\"examples\"] = updated_examples\n",
    "with open(\"../../data-files-updated/ReaSCAN-compositional-p3-rd/data-compositional-splits.txt\", \"w\") as fd:\n",
    "    json.dump(p3_rd_data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P1+P2+P3: Compositional Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Combine all of three together\n",
    "# We downsample it to make it trainable within reasonable time frame!\n",
    "p1_path_to_data = \"../../ReaSCAN-v1.0/ReaSCAN-compositional-p1/data-train.txt\"\n",
    "print(f\"Reading dataset from file: {p1_path_to_data}...\")\n",
    "p1_data_json = json.load(open(p1_path_to_data, \"r\"))\n",
    "\n",
    "p2_path_to_data = \"../../ReaSCAN-v1.0/ReaSCAN-compositional-p2/data-train.txt\"\n",
    "print(f\"Reading dataset from file: {p2_path_to_data}...\")\n",
    "p2_data_json = json.load(open(p2_path_to_data, \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3_path_to_data = \"../../ReaSCAN-v1.0/ReaSCAN-compositional-p3/data-train.txt\"\n",
    "print(f\"Reading dataset from file: {p3_path_to_data}...\")\n",
    "p3_data_json = json.load(open(p3_path_to_data, \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine them into a single big train!\n",
    "p1_examples = p1_data_json[\"examples\"][\"train\"]\n",
    "p2_examples = p2_data_json[\"examples\"][\"train\"]\n",
    "p3_data_json[\"examples\"][\"train\"].extend(p1_examples)\n",
    "p3_data_json[\"examples\"][\"train\"].extend(p2_examples)\n",
    "data_json = p3_data_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us downsample it to ?K\n",
    "len(data_json[\"examples\"][\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For generating the splits, we actually have to go through compositional splits first\n",
    "# and then consider random splits like dev and test. Because, we don't want things mixed up\n",
    "# in the dev and test. Dev and test should only contain commands that appear in the train,\n",
    "# so a total random partition at the end should work.\n",
    "\n",
    "# We do the splits step-by-step!\n",
    "id_example_map = OrderedDict({})\n",
    "id_splits_map = OrderedDict({})\n",
    "index = 0\n",
    "for example in data_json[\"examples\"][\"train\"]:\n",
    "    id_example_map[index] = example\n",
    "    id_splits_map[index] = set([]) # set of splits that this example belongs to.\n",
    "\n",
    "    # A1\n",
    "    if \"yellow,square\" in example['command']:\n",
    "        id_splits_map[index].add(\"a1_novel_color_attribute\")\n",
    "    \n",
    "    # A2\n",
    "    if example[\"derivation\"] == \"$OBJ_0\":\n",
    "        if \"red,square\" in example['command'] or \\\n",
    "            (example['situation']['placed_objects']['0']['object']['shape'] == \"square\" and \\\n",
    "             example['situation']['placed_objects']['0']['object']['color'] == \"red\"):\n",
    "            id_splits_map[index].add(\"a2_novel_color_attribute_visual\")\n",
    "    elif example[\"derivation\"] == \"$OBJ_0 ^ $OBJ_1\":\n",
    "        if \"red,square\" in example['command'] or \\\n",
    "            (example['situation']['placed_objects']['0']['object']['shape'] == \"square\" and \\\n",
    "             example['situation']['placed_objects']['0']['object']['color'] == \"red\") or \\\n",
    "            (example['situation']['placed_objects']['1']['object']['shape'] == \"square\" and \\\n",
    "             example['situation']['placed_objects']['1']['object']['color'] == \"red\"):\n",
    "            id_splits_map[index].add(\"a2_novel_color_attribute_visual\")\n",
    "    elif example[\"derivation\"] == \"$OBJ_0 ^ $OBJ_1 & $OBJ_2\":\n",
    "        if \"red,square\" in example['command'] or \\\n",
    "            (example['situation']['placed_objects']['0']['object']['shape'] == \"square\" and \\\n",
    "             example['situation']['placed_objects']['0']['object']['color'] == \"red\") or \\\n",
    "            (example['situation']['placed_objects']['1']['object']['shape'] == \"square\" and \\\n",
    "             example['situation']['placed_objects']['1']['object']['color'] == \"red\") or \\\n",
    "            (example['situation']['placed_objects']['2']['object']['shape'] == \"square\" and \\\n",
    "             example['situation']['placed_objects']['2']['object']['color'] == \"red\"):\n",
    "            id_splits_map[index].add(\"a2_novel_color_attribute_visual\")\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    # A3\n",
    "    if \"small,cylinder\" in example['command'] or \\\n",
    "        \"small,red,cylinder\" in example['command'] or \\\n",
    "        \"small,blue,cylinder\" in example['command'] or \\\n",
    "        \"small,yellow,cylinder\" in example['command'] or \\\n",
    "        \"small,green,cylinder\" in example['command']:\n",
    "        id_splits_map[index].add(\"a3_novel_size_attribute\")\n",
    "    \n",
    "    # B1: we default to active generation process, not holding out.\n",
    "    \n",
    "    # B2\n",
    "    if \"same,size\" in example['command'] and \"inside,of\" in example['command']:\n",
    "        id_splits_map[index].add(\"c_novel_relation_coexist\")\n",
    "    \n",
    "    if example['grammer_pattern'] == \"$OBJ_0 ^ $OBJ_1 & $OBJ_2 & $OBJ_3\":\n",
    "        id_splits_map[index].add(\"e_novel_clause_length\")\n",
    "    \n",
    "    # C1 and C2: we default to active generation process, not holding out.\n",
    "    \n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_distribution = OrderedDict({})\n",
    "splits_assignment = OrderedDict({})\n",
    "count = 0\n",
    "ccount = 0\n",
    "for index, splits in id_splits_map.items():\n",
    "    if len(splits) == 0:\n",
    "        count += 1\n",
    "        split = \"train\" # let us split this up later!\n",
    "        if split in splits_distribution.keys():\n",
    "            splits_distribution[split] += 1\n",
    "        else:\n",
    "            splits_distribution[split] = 1\n",
    "        \n",
    "        if split in splits_assignment:\n",
    "            splits_assignment[split].append(index)\n",
    "        else:\n",
    "            splits_assignment[split] = [index]\n",
    "    else:\n",
    "        ccount += 1\n",
    "        for split in splits:\n",
    "            if split in splits_distribution.keys():\n",
    "                splits_distribution[split] += 1\n",
    "            else:\n",
    "                splits_distribution[split] = 1\n",
    "                \n",
    "            if split in splits_assignment:\n",
    "                splits_assignment[split].append(index)\n",
    "            else:\n",
    "                splits_assignment[split] = [index]\n",
    "\n",
    "# Let us further segment train into dev and test!\n",
    "all_example_id = splits_assignment[\"train\"]\n",
    "gscan_dev_size = int(len(all_example_id)*0.01)\n",
    "gscan_test_size = int(len(all_example_id)*0.052)\n",
    "random.shuffle(all_example_id)\n",
    "train_example_id = all_example_id[:(-gscan_dev_size-gscan_test_size)]\n",
    "dev_example_id = all_example_id[(-gscan_dev_size-gscan_test_size):-gscan_dev_size]\n",
    "test_example_id = all_example_id[-gscan_dev_size:]\n",
    "splits_assignment[\"train\"] = train_example_id\n",
    "splits_assignment[\"dev\"] = dev_example_id\n",
    "splits_assignment[\"test\"] = test_example_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, all_ids in splits_assignment.items():\n",
    "    print(f\"for {split} split, we have {len(all_ids)} examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remake our data file accordingly.\n",
    "updated_examples = OrderedDict({})\n",
    "for split, all_ids in splits_assignment.items():\n",
    "    updated_examples[split] = []\n",
    "    for _id in all_ids:\n",
    "        updated_examples[split].append(id_example_map[_id])\n",
    "# save it to the disk\n",
    "data_json[\"examples\"] = updated_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data-files-updated/ReaSCAN-compositional/data-compositional-splits-all.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remake our data file accordingly.\n",
    "updated_examples = OrderedDict({})\n",
    "for split, all_ids in splits_assignment.items():\n",
    "    if split == \"train\" or split == \"dev\" or split == \"test\":\n",
    "        updated_examples[split] = []\n",
    "        for _id in all_ids:\n",
    "            updated_examples[split].append(id_example_map[_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it to the disk\n",
    "data_json[\"examples\"] = updated_examples\n",
    "with open(\"../../data-files-updated/ReaSCAN-compositional/data-compositional-splits-train.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P1+P2+P3: Compositional Splits Continue\n",
    "We need to make sure novel attribute splits actually require the attribute to reason, otherwise, it becomes less meaningful, and may cause accuracy inflation afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all of three together\n",
    "# We downsample it to make it trainable within reasonable time frame!\n",
    "path_to_data = \"../../data-files/ReaSCAN-compositional/data-compositional-splits-all.txt\"\n",
    "logger.info(f\"Reading dataset from file: {path_to_data}...\")\n",
    "data_json = json.load(open(path_to_data, \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json[\"examples\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a1 \n",
    "a1_attribute_example_filtered = []\n",
    "attribute_change = 0\n",
    "for example in data_json[\"examples\"]['a1_novel_color_attribute']:\n",
    "    if example['has_attribute_distractor']:\n",
    "        for k, v in example['object_expression'].items():\n",
    "            if \"yellow square\" in v:\n",
    "                if example['attribute_distractor_metadata'][0]['distractor_metadata'][0]['modified_obj'] == k:\n",
    "                    if example['attribute_distractor_metadata'][0]['distractor_metadata'][0]['modified_attribute'] == \"$COLOR\":\n",
    "                        a1_attribute_example_filtered += [example]\n",
    "                        attribute_change += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Actual examples for a1 = {attribute_change}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a2\n",
    "a2_attribute_example_filtered = []\n",
    "attribute_change = 0\n",
    "for example in data_json[\"examples\"]['a2_novel_color_attribute_visual']:\n",
    "    if \"red,square\" in example[\"command\"]:\n",
    "        if example['has_attribute_distractor']:\n",
    "            for k, v in example['object_expression'].items():\n",
    "                if \"red square\" in v:\n",
    "                    if example['attribute_distractor_metadata'][0]['distractor_metadata'][0]['modified_obj'] == k:\n",
    "                        if example['attribute_distractor_metadata'][0]['distractor_metadata'][0]['modified_attribute'] == \"$COLOR\":\n",
    "                            a2_attribute_example_filtered += [example]\n",
    "                            attribute_change += 1\n",
    "    else:\n",
    "        # this is for the visual part, we automatically added in.\n",
    "        a2_attribute_example_filtered += [example]\n",
    "        attribute_change += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Actual examples for a2 = {attribute_change}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a3\n",
    "a3_attribute_example_filtered = []\n",
    "attribute_change = 0\n",
    "for example in data_json[\"examples\"]['a3_novel_size_attribute']:\n",
    "    if example['has_attribute_distractor']:\n",
    "        for k, v in example['object_expression'].items():\n",
    "            if \"small\" in v and \"cylinder\" in v:\n",
    "                if example['attribute_distractor_metadata'][0]['distractor_metadata'][0]['modified_obj'] == k:\n",
    "                    if example['attribute_distractor_metadata'][0]['distractor_metadata'][0]['modified_attribute'] == \"$SIZE\":\n",
    "                        a3_attribute_example_filtered += [example]\n",
    "                        attribute_change += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Actual examples for a3 = {attribute_change}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1_attribute_example_filtered = []\n",
    "for example in data_json[\"examples\"]['b_novel_object_coexist']:\n",
    "    b1_attribute_example_filtered += [example]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(b1_attribute_example_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b2_attribute_example_filtered = []\n",
    "for example in data_json[\"examples\"]['c_novel_relation_coexist']:\n",
    "    b2_attribute_example_filtered += [example]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(b2_attribute_example_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b3_attribute_example_filtered = []\n",
    "for example in data_json[\"examples\"]['d_novel_object_relation_pair']:\n",
    "    b3_attribute_example_filtered += [example]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_test_example_filtered = []\n",
    "p2_test_example_filtered = []\n",
    "p3_test_example_filtered = []\n",
    "for example in data_json[\"examples\"][\"test\"]:\n",
    "    if example['derivation'] == \"$OBJ_0\":\n",
    "        p1_test_example_filtered += [example]\n",
    "    elif example['derivation'] == \"$OBJ_0 ^ $OBJ_1\":\n",
    "        p2_test_example_filtered += [example]\n",
    "    elif example['derivation'] == \"$OBJ_0 ^ $OBJ_1 & $OBJ_2\":\n",
    "        p3_test_example_filtered += [example]\n",
    "print(f\"p1 test example count={len(p1_test_example_filtered)}\")\n",
    "print(f\"p2 test example count={len(p2_test_example_filtered)}\")\n",
    "print(f\"p3 test example count={len(p3_test_example_filtered)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us write each splits into a different file, so it can be loaded much faster!\n",
    "data_json[\"examples\"] = {}\n",
    "data_json[\"examples\"][\"test\"] = a1_attribute_example_filtered\n",
    "with open(\"../../data-files-updated/ReaSCAN-compositional-a1/data-compositional.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json[\"examples\"] = {}\n",
    "data_json[\"examples\"][\"test\"] = a2_attribute_example_filtered\n",
    "with open(\"../../data-files-updated/ReaSCAN-compositional-a2/data-compositional.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json[\"examples\"] = {}\n",
    "data_json[\"examples\"][\"test\"] = a3_attribute_example_filtered\n",
    "with open(\"../../data-files-updated/ReaSCAN-compositional-a3/data-compositional.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json[\"examples\"] = {}\n",
    "data_json[\"examples\"][\"test\"] = b1_attribute_example_filtered\n",
    "with open(\"../../data-files-updated/ReaSCAN-compositional-b1/data-compositional.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json[\"examples\"] = {}\n",
    "data_json[\"examples\"][\"test\"] = b2_attribute_example_filtered\n",
    "with open(\"../../data-files-updated/ReaSCAN-compositional-b2/data-compositional.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json[\"examples\"] = {}\n",
    "data_json[\"examples\"][\"test\"] = b3_attribute_example_filtered\n",
    "with open(\"../../data-files-updated/ReaSCAN-compositional-b3/data-compositional.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us write each splits into a different file, so it can be loaded much faster!\n",
    "data_json[\"examples\"] = {}\n",
    "data_json[\"examples\"][\"test\"] = p1_test_example_filtered\n",
    "with open(\"../../data-files-updated/ReaSCAN-compositional-p1-test/data-compositional.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us write each splits into a different file, so it can be loaded much faster!\n",
    "data_json[\"examples\"] = {}\n",
    "data_json[\"examples\"][\"test\"] = p2_test_example_filtered\n",
    "with open(\"../../data-files-updated/ReaSCAN-compositional-p2-test/data-compositional.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us write each splits into a different file, so it can be loaded much faster!\n",
    "data_json[\"examples\"] = {}\n",
    "data_json[\"examples\"][\"test\"] = p3_test_example_filtered\n",
    "with open(\"../../data-files-updated/ReaSCAN-compositional-p3-test/data-compositional.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Novel Clause Length Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"../../data-files-updated/ReaSCAN-compositional-p4/data-train.txt\"\n",
    "logger.info(f\"Reading dataset from file: {path_to_data}...\")\n",
    "data_json = json.load(open(path_to_data, \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p4_test_example_filtered = data_json[\"examples\"][\"train\"]\n",
    "data_json[\"examples\"] = {}\n",
    "data_json[\"examples\"][\"test\"] = p4_test_example_filtered\n",
    "with open(\"../../data-files-updated/ReaSCAN-compositional-p4-test/data-compositional.txt\", \"w\") as fd:\n",
    "    json.dump(data_json, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_json[\"examples\"][\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore more possibilities of harder splits!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Command constant but more complex distractor sampling stratigies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test out the vocabulary\n",
    "intransitive_verbs = [\"walk\"]\n",
    "transitive_verbs = [\"push\", \"pull\"]\n",
    "adverbs = [\"while zigzagging\", \"while spinning\", \"cautiously\", \"hesitantly\"]\n",
    "nouns = [\"circle\", \"cylinder\", \"square\", \"box\"]\n",
    "color_adjectives = [\"red\", \"blue\", \"green\", \"yellow\"]\n",
    "size_adjectives = [\"big\", \"small\"]\n",
    "relative_pronouns = [\"that is\"]\n",
    "relation_clauses = [\"in the same row as\", \n",
    "                    \"in the same column as\", \n",
    "                    \"in the same color as\", \n",
    "                    \"in the same shape as\", \n",
    "                    \"in the same size as\",\n",
    "                    \"inside of\"]\n",
    "vocabulary = Vocabulary.initialize(intransitive_verbs=intransitive_verbs,\n",
    "                                   transitive_verbs=transitive_verbs, adverbs=adverbs, nouns=nouns,\n",
    "                                   color_adjectives=color_adjectives,\n",
    "                                   size_adjectives=size_adjectives, \n",
    "                                   relative_pronouns=relative_pronouns, \n",
    "                                   relation_clauses=relation_clauses)\n",
    "\n",
    "min_object_size = 1\n",
    "max_object_size = 4\n",
    "object_vocabulary = ObjectVocabulary(shapes=vocabulary.get_semantic_shapes(),\n",
    "                                     colors=vocabulary.get_semantic_colors(),\n",
    "                                     min_size=min_object_size, max_size=max_object_size)\n",
    "\n",
    "grammer = Grammer(vocabulary)\n",
    "\n",
    "simulator = Simulator(\n",
    "    object_vocabulary, vocabulary, \n",
    "    grid_size=6, \n",
    "    n_object_max=13,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us explore different distractor sampling!\n",
    "pattern = \"\"\n",
    "if pattern == \"p4\":\n",
    "    ReaSCAN_data_file = f\"ReaSCAN-compositional{pattern}/data-train.txt\"\n",
    "else:\n",
    "    ReaSCAN_data_file = f\"ReaSCAN-compositional{pattern}/data-compositional-splits.txt\"\n",
    "ReaSCAN_data_json = json.load(open(os.path.join(\"../../data-files-updated/\", ReaSCAN_data_file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_tasks = []\n",
    "\n",
    "per_command_world_target_count = 3 # Need to increase this for some tasks!\n",
    "per_command_world_retry_max = 200\n",
    "sampled_command_count = 5000\n",
    "\n",
    "examples = ReaSCAN_data_json[\"examples\"][\"train\"]\n",
    "random.shuffle(examples)\n",
    "sampled_examples = random.sample(examples, k=sampled_command_count)\n",
    "progress = 0\n",
    "\n",
    "for example_selected in sampled_examples:\n",
    "    \n",
    "    if example_selected['grammer_pattern'] != '$OBJ_0 ^ $OBJ_1 & $OBJ_2':\n",
    "        continue\n",
    "    \n",
    "    progress += 1\n",
    "    if progress % 10 == 0:\n",
    "        print(f\"count={len(synthetic_tasks)}\")\n",
    "        print(f\"progress={progress}\")\n",
    "    if len(synthetic_tasks) > 10000:\n",
    "        break # we get enough!\n",
    "        \n",
    "    rel_map = OrderedDict({})\n",
    "    for ele in example_selected[\"relation_map\"]:\n",
    "        rel_map[tuple(ele[0])] = ele[1]\n",
    "    example_struct = {'obj_pattern_map': example_selected[\"object_pattern_map\"],\n",
    "     'rel_map': rel_map,\n",
    "     'obj_map': example_selected[\"object_expression\"],\n",
    "     'grammer_pattern': example_selected['grammer_pattern'],\n",
    "     'adverb': example_selected['adverb_in_command'],\n",
    "     'verb': example_selected['verb_in_command']}\n",
    "    \n",
    "    obj_pattern_map = example_struct[\"obj_pattern_map\"]\n",
    "    rel_map = example_struct[\"rel_map\"]\n",
    "    obj_map = example_struct[\"obj_map\"]\n",
    "    grammer_pattern = example_struct[\"grammer_pattern\"]\n",
    "    verb = example_struct[\"verb\"]\n",
    "    adverb = example_struct[\"adverb\"]\n",
    "\n",
    "    for _ in range(per_command_world_target_count):\n",
    "        for i in range(per_command_world_retry_max): # retry essentially!\n",
    "            sampled_world = simulator.sample_situations_from_grounded_grammer(\n",
    "                    copy.deepcopy(grammer_pattern), \n",
    "                    copy.deepcopy(obj_pattern_map), \n",
    "                    copy.deepcopy(rel_map), \n",
    "                    copy.deepcopy(obj_map),\n",
    "                    is_plot=False,\n",
    "                    include_relation_distractor=True, \n",
    "                    include_attribute_distractor=False, \n",
    "                    include_isomorphism_distractor=False, \n",
    "                    include_random_distractor=True,\n",
    "                    full_relation_probability=1.0, # 0.5 seems to work as well!\n",
    "                    debug=False\n",
    "                )\n",
    "            \n",
    "            # print(sampled_world)\n",
    "            # _ = simulator._world.render_simple()\n",
    "            \n",
    "            assert len(sampled_world['obj_map']) == len(simulator._world.get_current_situation().to_representation()[\"placed_objects\"])\n",
    "\n",
    "            graph = ReaSCANGraph(\n",
    "                objects=sampled_world[\"obj_map\"], \n",
    "                object_patterns=sampled_world[\"obj_pattern_map\"], \n",
    "                vocabulary=vocabulary,\n",
    "                positions=sampled_world[\"pos_map\"], \n",
    "                referred_object=sampled_world[\"referred_obj\"],\n",
    "                debug=False\n",
    "            )\n",
    "\n",
    "            pattern_graph = ReaSCANGraph(\n",
    "                objects=obj_map, \n",
    "                object_patterns=None,\n",
    "                vocabulary=vocabulary,\n",
    "                relations=rel_map, \n",
    "                referred_object='$OBJ_0', \n",
    "                debug=False\n",
    "            )\n",
    "\n",
    "            potential_referent_target = graph.find_referred_object_super_fast(\n",
    "                pattern_graph, referred_object='$OBJ_0', \n",
    "                debug=False\n",
    "            )\n",
    "\n",
    "            if len(potential_referent_target) == 1 and '$OBJ_0' in potential_referent_target:\n",
    "                # test_unique_find += 1\n",
    "                # print(f\"{test_unique_find} / {i+1} unique solution find!\")\n",
    "\n",
    "                # Form the command with grounded determiners!\n",
    "                obj_determiner_map = graph.find_determiners(\n",
    "                    pattern_graph, \n",
    "                    referred_object='$OBJ_0', \n",
    "                    debug=False,\n",
    "                )\n",
    "                command_str = grammer.repre_str_command(\n",
    "                    grammer_pattern, rel_map, obj_map, \n",
    "                    obj_determiner_map, \n",
    "                    verb,\n",
    "                    adverb,\n",
    "                )\n",
    "\n",
    "                # Form the golden label for the action list!\n",
    "                is_transitive = False\n",
    "                if verb in simulator.vocabulary.get_transitive_verbs():\n",
    "                    is_transitive = True\n",
    "                # Direct walk.\n",
    "                action = \"walk\" # this is definit!\n",
    "                primitive_command = simulator.vocabulary.translate_word(action)\n",
    "                target_position = sampled_world[\"situation\"].target_object.position\n",
    "                simulator._world.go_to_position(\n",
    "                    position=target_position, manner=adverb, \n",
    "                    primitive_command=primitive_command\n",
    "                )\n",
    "                # Object actions.\n",
    "                if is_transitive:\n",
    "                    semantic_action = simulator.vocabulary.translate_word(verb)\n",
    "                    simulator._world.move_object_to_wall(action=semantic_action, manner=adverb)\n",
    "                target_commands, _ = simulator._world.get_current_observations()\n",
    "\n",
    "                has_relation_distractor = False\n",
    "                full_relation_distractor = True\n",
    "                for rel_bool in sampled_world[\"distractor_switch_map\"][\"relation\"]:\n",
    "                    if rel_bool:\n",
    "                        has_relation_distractor = True\n",
    "                    else:\n",
    "                        full_relation_distractor = False\n",
    "\n",
    "                # Save all relevant information for a task.\n",
    "                task_struct = OrderedDict({\n",
    "                    \"command\": \",\".join(command_str.split(\" \")),\n",
    "                    \"grammer_pattern\": grammer_pattern,\n",
    "                    \"meaning\": \",\".join(command_str.split(\" \")),\n",
    "                    \"derivation\": grammer_pattern,\n",
    "                    \"situation\": sampled_world[\"situation\"].to_representation(),\n",
    "                    \"target_commands\": \",\".join(target_commands),\n",
    "                    \"verb_in_command\": verb,\n",
    "                    \"adverb_in_command\": adverb,\n",
    "                    \"referred_target\": obj_map[\"$OBJ_0\"],\n",
    "                    \"object_pattern_map\": obj_pattern_map,\n",
    "                    \"relation_map\": [(k, v) for k, v in rel_map.items()],\n",
    "                    \"object_expression\": obj_map,\n",
    "                    \"n_object\": len(sampled_world[\"obj_map\"]),\n",
    "                    \"n_distractor\": len(sampled_world[\"obj_map\"])-len(obj_map),\n",
    "                    \"full_relation_distractor\": full_relation_distractor,\n",
    "                    \"has_relation_distractor\": has_relation_distractor,\n",
    "                    \"has_attribute_distractor\": sampled_world[\"distractor_switch_map\"][\"attribute\"],\n",
    "                    \"has_isomorphism_distractor\": sampled_world[\"distractor_switch_map\"][\"isomorphism\"],\n",
    "                    \"has_random_distractor\": True if sampled_world[\"n_random_distractor\"] != -1 else False,\n",
    "                    \"n_random_distractor\": sampled_world[\"n_random_distractor\"] if sampled_world[\"n_random_distractor\"] != -1 else 0,\n",
    "                    \"relation_distractor_metadata\": sampled_world[\"relation_distractor_metadata\"],\n",
    "                    \"attribute_distractor_metadata\": sampled_world[\"attribute_distractor_metadata\"],\n",
    "                    \"isomorphism_distractor_metadata\": sampled_world[\"isomorphism_distractor_metadata\"],\n",
    "                    \"random_distractor_metadata\": sampled_world[\"random_distractor_metadata\"],\n",
    "                })\n",
    "                synthetic_tasks += [task_struct]\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_tasks = {\n",
    "    \"test\" : synthetic_tasks\n",
    "}\n",
    "dataset_representation = {\n",
    "    \"grid_size\": 6,\n",
    "    \"type_grammar\": \"ReaSCAN-Grammer\",\n",
    "    \"min_object_size\": 1,\n",
    "    \"max_object_size\": 4,\n",
    "    \"percentage_train\": 0.0,\n",
    "    \"examples\": synthetic_tasks,\n",
    "    \"intransitive_verbs\": intransitive_verbs,\n",
    "    \"transitive_verbs\": transitive_verbs,\n",
    "    \"adverbs\": adverbs,\n",
    "    \"nouns\": nouns,\n",
    "    \"color_adjectives\": color_adjectives,\n",
    "    \"size_adjectives\": size_adjectives,\n",
    "    \"relative_pronouns\": relative_pronouns,\n",
    "    \"relation_clauses\": relation_clauses,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_name = \"-f1\"\n",
    "with open(f\"../../data-files-updated/ReaSCAN-compositional{split_name}/data-compositional-splits.txt\", \"w\") as fd:\n",
    "    json.dump(dataset_representation, fd, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unseen co-occurence of relations and objects but with seen atomic concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently, we hard-code the pattern!\n",
    "grammer_pattern = '$OBJ_0 ^ $OBJ_1 & $OBJ_2'\n",
    "logger.info(f\"Including pattern:= {grammer_pattern}...\")\n",
    "# Sampling relations\n",
    "relations = grammer.sample_object_relation_grammer(\n",
    "    '$OBJ_0', \n",
    "    grammer.build_dependency_graph(grammer_pattern))\n",
    "command_structs = {}\n",
    "for relation in relations:\n",
    "    obj_pattern_map = relation[0]\n",
    "    rel_map = relation[1]\n",
    "    grammer_bindings = grammer.grounding_grammer_with_vocabulary(grammer_pattern, obj_pattern_map, rel_map)\n",
    "    for obj_map in grammer_bindings:\n",
    "        # here, we also sample the verb and adverb bindings!\n",
    "        adverb_enhance_list = vocabulary.get_adverbs()\n",
    "        adverb_enhance_list += [\"\"]\n",
    "        command_struct = OrderedDict({\n",
    "            \"obj_pattern_map\" : obj_pattern_map,\n",
    "            \"rel_map\" : rel_map,\n",
    "            \"obj_map\" : obj_map,\n",
    "            \"grammer_pattern\" : grammer_pattern,\n",
    "            \"adverb\" : random.choice(adverb_enhance_list),\n",
    "            \"verb\" : random.choice(vocabulary.get_transitive_verbs() + vocabulary.get_intransitive_verbs()),\n",
    "        })\n",
    "        command_str = grammer.repre_str_command(\n",
    "            grammer_pattern, rel_map, obj_map, \n",
    "            {\"$OBJ_0\" : \"the\", \"$OBJ_1\" : \"a\", \"$OBJ_2\" : \"a\"}, \n",
    "            command_struct[\"verb\"],\n",
    "            command_struct[\"adverb\"],\n",
    "        )\n",
    "        command_structs[command_str] = command_struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_command_structs = {}\n",
    "seen_concepts = {} # add in seen concepts, so we can select concepts that are seen, but new composites!\n",
    "seen_object_co = set([])\n",
    "seen_rel_co = set([])\n",
    "seen_rel_obj_co = set([])\n",
    "\n",
    "for example_selected in ReaSCAN_data_json[\"examples\"][\"train\"]:\n",
    "    rel_map = OrderedDict({})\n",
    "    for ele in example_selected[\"relation_map\"]:\n",
    "        rel_map[tuple(ele[0])] = ele[1]\n",
    "    example_struct = OrderedDict({\n",
    "        'obj_pattern_map': example_selected[\"object_pattern_map\"],\n",
    "        'rel_map': rel_map,\n",
    "        'obj_map': example_selected[\"object_expression\"],\n",
    "        'grammer_pattern': example_selected['grammer_pattern'],\n",
    "        'adverb': example_selected['adverb_in_command'],\n",
    "        'verb': example_selected['verb_in_command']\n",
    "    })\n",
    "    obj_co = []\n",
    "    for k, v in example_selected[\"object_expression\"].items():\n",
    "        if v not in seen_concepts:\n",
    "            seen_concepts[v] = 1\n",
    "        else:\n",
    "            seen_concepts[v] += 1\n",
    "        obj_co += [v]\n",
    "    obj_co.sort()\n",
    "    seen_object_co.add(tuple(obj_co))\n",
    "    \n",
    "    rel_co = []\n",
    "    for k, v in rel_map.items():\n",
    "        if v not in seen_concepts:\n",
    "            seen_concepts[v] = 1\n",
    "        else:\n",
    "            seen_concepts[v] += 1\n",
    "        rel_co += [v]\n",
    "    rel_co.sort()\n",
    "    seen_rel_co.add(tuple(rel_co))\n",
    "    \n",
    "    if example_selected['grammer_pattern'] == \"$OBJ_0 ^ $OBJ_1 & $OBJ_2\":\n",
    "        pair = (rel_map[(\"$OBJ_0\", \"$OBJ_1\")], example_selected[\"object_expression\"][\"$OBJ_1\"])\n",
    "        seen_rel_obj_co.add(pair)\n",
    "        pair = (rel_map[(\"$OBJ_0\", \"$OBJ_2\")], example_selected[\"object_expression\"][\"$OBJ_2\"])\n",
    "        seen_rel_obj_co.add(pair)\n",
    "    elif example_selected['grammer_pattern'] == \"$OBJ_0 ^ $OBJ_1\":\n",
    "        pair = (rel_map[(\"$OBJ_0\", \"$OBJ_1\")], example_selected[\"object_expression\"][\"$OBJ_1\"])\n",
    "        seen_rel_obj_co.add(pair)\n",
    "\n",
    "    # if example_selected['grammer_pattern'] == \"$OBJ_0 ^ $OBJ_1 & $OBJ_2\":\n",
    "    command_str = grammer.repre_str_command(\n",
    "        example_selected['grammer_pattern'], rel_map, example_selected[\"object_expression\"], \n",
    "        {\"$OBJ_0\" : \"the\", \"$OBJ_1\" : \"a\", \"$OBJ_2\" : \"a\"}, \n",
    "        example_selected['verb_in_command'],\n",
    "        example_selected['adverb_in_command'],\n",
    "    )\n",
    "    seen_command_structs[command_str] = example_struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_obj_co_command_structs = []\n",
    "unseen_obj_rel_co_command_structs = []\n",
    "unseen_rel_co_command_structs = []\n",
    "unseen_obj_rel_pair_command_structs = []\n",
    "for k, v in command_structs.items():\n",
    "\n",
    "    if k not in seen_command_structs:\n",
    "\n",
    "        # we need to ensure concepts are seen before though!\n",
    "        concept_seen = True\n",
    "        obj_co = []\n",
    "        for kk, vv in v['obj_map'].items():\n",
    "            if vv not in seen_concepts.keys():\n",
    "                concept_seen = False\n",
    "                break\n",
    "            obj_co += [vv]\n",
    "        \n",
    "        rel_co = []\n",
    "        for kk, vv in v['rel_map'].items():\n",
    "            if vv not in seen_concepts.keys():\n",
    "                concept_seen = False\n",
    "                break\n",
    "            rel_co += [vv]\n",
    "\n",
    "        pair = (v['rel_map'][(\"$OBJ_0\", \"$OBJ_1\")], v[\"obj_map\"][\"$OBJ_1\"])\n",
    "        rel_obj_co_1 = pair\n",
    "        pair = (v['rel_map'][(\"$OBJ_0\", \"$OBJ_2\")], v[\"obj_map\"][\"$OBJ_2\"])\n",
    "        rel_obj_co_2 = pair\n",
    "\n",
    "        if concept_seen:\n",
    "            obj_co.sort()\n",
    "            obj_co = tuple(obj_co)\n",
    "            rel_co.sort()\n",
    "            rel_co = tuple(rel_co)\n",
    "            if rel_co not in seen_rel_co and obj_co in seen_object_co:\n",
    "                unseen_rel_co_command_structs += [v]\n",
    "            if obj_co not in seen_object_co and rel_co in seen_rel_co:\n",
    "                unseen_obj_co_command_structs += [v]\n",
    "            if obj_co not in seen_object_co and rel_co not in seen_rel_co:\n",
    "                unseen_obj_rel_co_command_structs += [v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_tasks = []\n",
    "progress = 0\n",
    "\n",
    "per_command_world_target_count = 3 # Need to increase this for some tasks!\n",
    "per_command_world_retry_max = 200\n",
    "sampled_command_count = 5000\n",
    "\n",
    "for example_selected in unseen_obj_co_command_structs:\n",
    "\n",
    "    progress += 1\n",
    "    if progress % 50 == 0:\n",
    "        print(f\"count={len(synthetic_tasks)}\")\n",
    "        print(f\"progress={progress}\")\n",
    "    if len(synthetic_tasks) > 10000:\n",
    "        break # we get enough!\n",
    "\n",
    "    obj_pattern_map = example_selected[\"obj_pattern_map\"]\n",
    "    rel_map = example_selected[\"rel_map\"]\n",
    "    obj_map = example_selected[\"obj_map\"]\n",
    "    grammer_pattern = example_selected[\"grammer_pattern\"]\n",
    "    verb = example_selected[\"verb\"]\n",
    "    adverb = example_selected[\"adverb\"]\n",
    "\n",
    "    for _ in range(per_command_world_target_count):\n",
    "        for i in range(per_command_world_retry_max): # retry essentially!\n",
    "            sampled_world = simulator.sample_situations_from_grounded_grammer(\n",
    "                    copy.deepcopy(grammer_pattern), \n",
    "                    copy.deepcopy(obj_pattern_map), \n",
    "                    copy.deepcopy(rel_map), \n",
    "                    copy.deepcopy(obj_map),\n",
    "                    is_plot=False,\n",
    "                    include_relation_distractor=True, \n",
    "                    include_attribute_distractor=True, \n",
    "                    include_isomorphism_distractor=True, \n",
    "                    include_random_distractor=True,\n",
    "                    full_relation_probability=1.0, # 0.5 seems to work as well!\n",
    "                    debug=False\n",
    "                )\n",
    "\n",
    "            assert len(sampled_world['obj_map']) == len(simulator._world.get_current_situation().to_representation()[\"placed_objects\"])\n",
    "\n",
    "            graph = ReaSCANGraph(\n",
    "                objects=sampled_world[\"obj_map\"], \n",
    "                object_patterns=sampled_world[\"obj_pattern_map\"], \n",
    "                vocabulary=vocabulary,\n",
    "                positions=sampled_world[\"pos_map\"], \n",
    "                referred_object=sampled_world[\"referred_obj\"],\n",
    "                debug=False\n",
    "            )\n",
    "\n",
    "            pattern_graph = ReaSCANGraph(\n",
    "                objects=obj_map, \n",
    "                object_patterns=None,\n",
    "                vocabulary=vocabulary,\n",
    "                relations=rel_map, \n",
    "                referred_object='$OBJ_0', \n",
    "                debug=False\n",
    "            )\n",
    "\n",
    "            potential_referent_target = graph.find_referred_object_super_fast(\n",
    "                pattern_graph, referred_object='$OBJ_0', \n",
    "                debug=False\n",
    "            )\n",
    "\n",
    "            if len(potential_referent_target) == 1 and '$OBJ_0' in potential_referent_target:\n",
    "                # print(f\"{test_unique_find} / {i+1} unique solution find!\")\n",
    "                # Form the command with grounded determiners!\n",
    "                obj_determiner_map = graph.find_determiners(\n",
    "                    pattern_graph, \n",
    "                    referred_object='$OBJ_0', \n",
    "                    debug=False,\n",
    "                )\n",
    "                command_str = grammer.repre_str_command(\n",
    "                    grammer_pattern, rel_map, obj_map, \n",
    "                    obj_determiner_map, \n",
    "                    verb,\n",
    "                    adverb,\n",
    "                )\n",
    "\n",
    "                # Form the golden label for the action list!\n",
    "                is_transitive = False\n",
    "                if verb in simulator.vocabulary.get_transitive_verbs():\n",
    "                    is_transitive = True\n",
    "                # Direct walk.\n",
    "                action = \"walk\" # this is definit!\n",
    "                primitive_command = simulator.vocabulary.translate_word(action)\n",
    "                target_position = sampled_world[\"situation\"].target_object.position\n",
    "                simulator._world.go_to_position(\n",
    "                    position=target_position, manner=adverb, \n",
    "                    primitive_command=primitive_command\n",
    "                )\n",
    "                # Object actions.\n",
    "                if is_transitive:\n",
    "                    semantic_action = simulator.vocabulary.translate_word(verb)\n",
    "                    simulator._world.move_object_to_wall(action=semantic_action, manner=adverb)\n",
    "                target_commands, _ = simulator._world.get_current_observations()\n",
    "\n",
    "                has_relation_distractor = False\n",
    "                full_relation_distractor = True\n",
    "                for rel_bool in sampled_world[\"distractor_switch_map\"][\"relation\"]:\n",
    "                    if rel_bool:\n",
    "                        has_relation_distractor = True\n",
    "                    else:\n",
    "                        full_relation_distractor = False\n",
    "\n",
    "                # Save all relevant information for a task.\n",
    "                task_struct = OrderedDict({\n",
    "                    \"command\": \",\".join(command_str.split(\" \")),\n",
    "                    \"grammer_pattern\": grammer_pattern,\n",
    "                    \"meaning\": \",\".join(command_str.split(\" \")),\n",
    "                    \"derivation\": grammer_pattern,\n",
    "                    \"situation\": sampled_world[\"situation\"].to_representation(),\n",
    "                    \"target_commands\": \",\".join(target_commands),\n",
    "                    \"verb_in_command\": verb,\n",
    "                    \"adverb_in_command\": adverb,\n",
    "                    \"referred_target\": obj_map[\"$OBJ_0\"],\n",
    "                    \"object_pattern_map\": obj_pattern_map,\n",
    "                    \"relation_map\": [(k, v) for k, v in rel_map.items()],\n",
    "                    \"object_expression\": obj_map,\n",
    "                    \"n_object\": len(sampled_world[\"obj_map\"]),\n",
    "                    \"n_distractor\": len(sampled_world[\"obj_map\"])-len(obj_map),\n",
    "                    \"full_relation_distractor\": full_relation_distractor,\n",
    "                    \"has_relation_distractor\": has_relation_distractor,\n",
    "                    \"has_attribute_distractor\": sampled_world[\"distractor_switch_map\"][\"attribute\"],\n",
    "                    \"has_isomorphism_distractor\": sampled_world[\"distractor_switch_map\"][\"isomorphism\"],\n",
    "                    \"has_random_distractor\": True if sampled_world[\"n_random_distractor\"] != -1 else False,\n",
    "                    \"n_random_distractor\": sampled_world[\"n_random_distractor\"] if sampled_world[\"n_random_distractor\"] != -1 else 0,\n",
    "                    \"relation_distractor_metadata\": sampled_world[\"relation_distractor_metadata\"],\n",
    "                    \"attribute_distractor_metadata\": sampled_world[\"attribute_distractor_metadata\"],\n",
    "                    \"isomorphism_distractor_metadata\": sampled_world[\"isomorphism_distractor_metadata\"],\n",
    "                    \"random_distractor_metadata\": sampled_world[\"random_distractor_metadata\"],\n",
    "                })\n",
    "                synthetic_tasks += [task_struct]\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_tasks = {\n",
    "    \"test\" : synthetic_tasks\n",
    "}\n",
    "dataset_representation = {\n",
    "    \"grid_size\": 6,\n",
    "    \"type_grammar\": \"ReaSCAN-Grammer\",\n",
    "    \"min_object_size\": 1,\n",
    "    \"max_object_size\": 4,\n",
    "    \"percentage_train\": 0.0,\n",
    "    \"examples\": synthetic_tasks,\n",
    "    \"intransitive_verbs\": intransitive_verbs,\n",
    "    \"transitive_verbs\": transitive_verbs,\n",
    "    \"adverbs\": adverbs,\n",
    "    \"nouns\": nouns,\n",
    "    \"color_adjectives\": color_adjectives,\n",
    "    \"size_adjectives\": size_adjectives,\n",
    "    \"relative_pronouns\": relative_pronouns,\n",
    "    \"relation_clauses\": relation_clauses,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_name = \"-d2\"\n",
    "with open(f\"../../data-files-updated/ReaSCAN-compositional{split_name}/data-compositional-splits.txt\", \"w\") as fd:\n",
    "    json.dump(dataset_representation, fd, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two \"that is\" clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_command_structs = []\n",
    "for example_selected in ReaSCAN_data_json[\"examples\"][\"train\"]:\n",
    "    rel_map = OrderedDict({})\n",
    "    for ele in example_selected[\"relation_map\"]:\n",
    "        rel_map[tuple(ele[0])] = ele[1]\n",
    "    example_struct = OrderedDict({\n",
    "        'obj_pattern_map': example_selected[\"object_pattern_map\"],\n",
    "        'rel_map': rel_map,\n",
    "        'obj_map': example_selected[\"object_expression\"],\n",
    "        'grammer_pattern': example_selected['grammer_pattern'], # force it to be double recursive\n",
    "        'adverb': example_selected['adverb_in_command'],\n",
    "        'verb': example_selected['verb_in_command']\n",
    "    })\n",
    "    \n",
    "    # the second object cannot be box!\n",
    "    if example_struct['grammer_pattern'] == '$OBJ_0 ^ $OBJ_1 & $OBJ_2':\n",
    "        if \"box\" not in example_struct[\"obj_map\"]['$OBJ_1']:\n",
    "            \n",
    "            # other filters as well!\n",
    "            if rel_map[(\"$OBJ_0\", \"$OBJ_1\")] in [\"$SAME_ROW\", \"$SAME_COLUMN\"] and \\\n",
    "                rel_map[(\"$OBJ_0\", \"$OBJ_2\")] in [\"$SAME_ROW\", \"$SAME_COLUMN\"]:\n",
    "                example_struct['grammer_pattern'] = '$OBJ_0 ^ $OBJ_1 ^ $OBJ_2'\n",
    "                rel_map_new = OrderedDict({})\n",
    "                rel_map_new[(\"$OBJ_0\", \"$OBJ_1\")] = example_struct['rel_map'][(\"$OBJ_0\", \"$OBJ_1\")]\n",
    "                rel_map_new[(\"$OBJ_1\", \"$OBJ_2\")] = example_struct['rel_map'][(\"$OBJ_0\", \"$OBJ_2\")]\n",
    "                example_struct['rel_map'] = rel_map_new\n",
    "                possible_command_structs += [example_struct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_tasks = []\n",
    "progress = 0\n",
    "for example_selected in possible_command_structs:\n",
    "\n",
    "    progress += 1\n",
    "    if progress % 10 == 0:\n",
    "        print(f\"count={len(synthetic_tasks)}\")\n",
    "        print(f\"progress={progress}\")\n",
    "    if len(synthetic_tasks) > 8000:\n",
    "        break # we get enough!\n",
    "        \n",
    "    obj_pattern_map = example_selected[\"obj_pattern_map\"]\n",
    "    rel_map = example_selected[\"rel_map\"]\n",
    "    obj_map = example_selected[\"obj_map\"]\n",
    "    grammer_pattern = example_selected[\"grammer_pattern\"]\n",
    "    verb = example_selected[\"verb\"]\n",
    "    adverb = example_selected[\"adverb\"]\n",
    "\n",
    "    for _ in range(per_command_world_target_count):\n",
    "        for i in range(per_command_world_retry_max): # retry essentially!\n",
    "            sampled_world = simulator.sample_situations_from_grounded_grammer(\n",
    "                    copy.deepcopy(grammer_pattern), \n",
    "                    copy.deepcopy(obj_pattern_map), \n",
    "                    copy.deepcopy(rel_map), \n",
    "                    copy.deepcopy(obj_map),\n",
    "                    is_plot=False,\n",
    "                    include_relation_distractor=True, \n",
    "                    include_attribute_distractor=True, \n",
    "                    include_isomorphism_distractor=True, \n",
    "                    include_random_distractor=True,\n",
    "                    full_relation_probability=1.0, # 0.5 seems to work as well!\n",
    "                    debug=False\n",
    "                )\n",
    "\n",
    "            assert len(sampled_world['obj_map']) == len(simulator._world.get_current_situation().to_representation()[\"placed_objects\"])\n",
    "\n",
    "            graph = ReaSCANGraph(\n",
    "                objects=sampled_world[\"obj_map\"], \n",
    "                object_patterns=sampled_world[\"obj_pattern_map\"], \n",
    "                vocabulary=vocabulary,\n",
    "                positions=sampled_world[\"pos_map\"], \n",
    "                referred_object=sampled_world[\"referred_obj\"],\n",
    "                debug=False\n",
    "            )\n",
    "\n",
    "            pattern_graph = ReaSCANGraph(\n",
    "                objects=obj_map, \n",
    "                object_patterns=None,\n",
    "                vocabulary=vocabulary,\n",
    "                relations=rel_map, \n",
    "                referred_object='$OBJ_0', \n",
    "                debug=False\n",
    "            )\n",
    "\n",
    "            potential_referent_target = graph.find_referred_object_super_fast(\n",
    "                pattern_graph, referred_object='$OBJ_0', \n",
    "                pattern=grammer_pattern,\n",
    "                debug=False\n",
    "            )\n",
    "\n",
    "            if len(potential_referent_target) == 1 and '$OBJ_0' in potential_referent_target:\n",
    "                # print(f\"{test_unique_find} / {i+1} unique solution find!\")\n",
    "                # Form the command with grounded determiners!\n",
    "                obj_determiner_map = graph.find_determiners(\n",
    "                    pattern_graph, \n",
    "                    referred_object='$OBJ_0', \n",
    "                    debug=False,\n",
    "                )\n",
    "                command_str = grammer.repre_str_command(\n",
    "                    grammer_pattern, rel_map, obj_map, \n",
    "                    obj_determiner_map, \n",
    "                    verb,\n",
    "                    adverb,\n",
    "                )\n",
    "\n",
    "                # Form the golden label for the action list!\n",
    "                is_transitive = False\n",
    "                if verb in simulator.vocabulary.get_transitive_verbs():\n",
    "                    is_transitive = True\n",
    "                # Direct walk.\n",
    "                action = \"walk\" # this is definit!\n",
    "                primitive_command = simulator.vocabulary.translate_word(action)\n",
    "                target_position = sampled_world[\"situation\"].target_object.position\n",
    "                simulator._world.go_to_position(\n",
    "                    position=target_position, manner=adverb, \n",
    "                    primitive_command=primitive_command\n",
    "                )\n",
    "                # Object actions.\n",
    "                if is_transitive:\n",
    "                    semantic_action = simulator.vocabulary.translate_word(verb)\n",
    "                    simulator._world.move_object_to_wall(action=semantic_action, manner=adverb)\n",
    "                target_commands, _ = simulator._world.get_current_observations()\n",
    "\n",
    "                has_relation_distractor = False\n",
    "                full_relation_distractor = True\n",
    "                for rel_bool in sampled_world[\"distractor_switch_map\"][\"relation\"]:\n",
    "                    if rel_bool:\n",
    "                        has_relation_distractor = True\n",
    "                    else:\n",
    "                        full_relation_distractor = False\n",
    "\n",
    "                # Save all relevant information for a task.\n",
    "                task_struct = OrderedDict({\n",
    "                    \"command\": \",\".join(command_str.split(\" \")),\n",
    "                    \"grammer_pattern\": grammer_pattern,\n",
    "                    \"meaning\": \",\".join(command_str.split(\" \")),\n",
    "                    \"derivation\": grammer_pattern,\n",
    "                    \"situation\": sampled_world[\"situation\"].to_representation(),\n",
    "                    \"target_commands\": \",\".join(target_commands),\n",
    "                    \"verb_in_command\": verb,\n",
    "                    \"adverb_in_command\": adverb,\n",
    "                    \"referred_target\": obj_map[\"$OBJ_0\"],\n",
    "                    \"object_pattern_map\": obj_pattern_map,\n",
    "                    \"relation_map\": [(k, v) for k, v in rel_map.items()],\n",
    "                    \"object_expression\": obj_map,\n",
    "                    \"n_object\": len(sampled_world[\"obj_map\"]),\n",
    "                    \"n_distractor\": len(sampled_world[\"obj_map\"])-len(obj_map),\n",
    "                    \"full_relation_distractor\": full_relation_distractor,\n",
    "                    \"has_relation_distractor\": has_relation_distractor,\n",
    "                    \"has_attribute_distractor\": sampled_world[\"distractor_switch_map\"][\"attribute\"],\n",
    "                    \"has_isomorphism_distractor\": sampled_world[\"distractor_switch_map\"][\"isomorphism\"],\n",
    "                    \"has_random_distractor\": True if sampled_world[\"n_random_distractor\"] != -1 else False,\n",
    "                    \"n_random_distractor\": sampled_world[\"n_random_distractor\"] if sampled_world[\"n_random_distractor\"] != -1 else 0,\n",
    "                    \"relation_distractor_metadata\": sampled_world[\"relation_distractor_metadata\"],\n",
    "                    \"attribute_distractor_metadata\": sampled_world[\"attribute_distractor_metadata\"],\n",
    "                    \"isomorphism_distractor_metadata\": sampled_world[\"isomorphism_distractor_metadata\"],\n",
    "                    \"random_distractor_metadata\": sampled_world[\"random_distractor_metadata\"],\n",
    "                })\n",
    "                synthetic_tasks += [task_struct]\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_tasks = {\n",
    "    \"test\" : synthetic_tasks\n",
    "}\n",
    "dataset_representation = {\n",
    "    \"grid_size\": 6,\n",
    "    \"type_grammar\": \"ReaSCAN-Grammer\",\n",
    "    \"min_object_size\": 1,\n",
    "    \"max_object_size\": 4,\n",
    "    \"percentage_train\": 0.0,\n",
    "    \"examples\": synthetic_tasks,\n",
    "    \"intransitive_verbs\": intransitive_verbs,\n",
    "    \"transitive_verbs\": transitive_verbs,\n",
    "    \"adverbs\": adverbs,\n",
    "    \"nouns\": nouns,\n",
    "    \"color_adjectives\": color_adjectives,\n",
    "    \"size_adjectives\": size_adjectives,\n",
    "    \"relative_pronouns\": relative_pronouns,\n",
    "    \"relation_clauses\": relation_clauses,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_name = \"-e\"\n",
    "with open(f\"../../data-files-updated/ReaSCAN-compositional{split_name}/data-compositional-splits.txt\", \"w\") as fd:\n",
    "    json.dump(dataset_representation, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
