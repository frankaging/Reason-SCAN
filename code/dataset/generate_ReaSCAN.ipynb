{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from collections import namedtuple, OrderedDict\n",
    "import itertools\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "from typing import List\n",
    "from typing import Dict\n",
    "import random\n",
    "from itertools import product\n",
    "import copy\n",
    "import re\n",
    "import random\n",
    "import hashlib\n",
    "import pathlib\n",
    "import json\n",
    "import matplotlib as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "os.environ['QT_QPA_PLATFORM']='offscreen'\n",
    "plt.rcParams[\"font.family\"] = \"DejaVu Serif\"\n",
    "font = {'family' : 'DejaVu Serif',\n",
    "        'size'   : 20}\n",
    "plt.rc('font', **font)\n",
    "import plotly.tools as tls\n",
    "\n",
    "from utils import one_hot\n",
    "from utils import generate_possible_object_names\n",
    "from utils import numpy_array_to_image\n",
    "\n",
    "from vocabulary import *\n",
    "from object_vocabulary import *\n",
    "from world import *\n",
    "from grammer import *\n",
    "from simulator import *\n",
    "from relation_graph import *\n",
    "\n",
    "import logging\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers.\n",
    "def get_relation_statistics(command_structs):\n",
    "    \"\"\"\n",
    "    Return a dictionary, (relation, position) with counts\n",
    "    \"\"\"\n",
    "    stats = {}\n",
    "    for i in range(2): # at max 2!\n",
    "        stats[f\"position-{i}\"] = {}\n",
    "    for command in command_structs:\n",
    "        pos_id = 0\n",
    "        for k, v in command[\"rel_map\"].items():\n",
    "            if v in stats[f\"position-{pos_id}\"].keys():\n",
    "                stats[f\"position-{pos_id}\"][v] += 1\n",
    "            else:\n",
    "                stats[f\"position-{pos_id}\"][v] = 1\n",
    "            pos_id += 1\n",
    "    return stats\n",
    "\n",
    "\n",
    "def get_attribute_statistics(command_structs, include_keywords=[\"circle\", \"cylinder\", \"square\", \"box\", \"object\"]):\n",
    "    \n",
    "    stats = {}\n",
    "    # for k, v in command_structs[0][\"obj_map\"].items():\n",
    "    #     stats[k] = {} # we can do it in object level!\n",
    "    for i in range(3): # at max 2!\n",
    "        stats[f\"$OBJ_{i}\"] = {}\n",
    "        \n",
    "    for command in command_structs:\n",
    "        for k, v in command[\"obj_map\"].items():\n",
    "            for keyword in include_keywords:\n",
    "                keyword_list = keyword.split(\" \") # in case there are a couple!\n",
    "                match = True\n",
    "                for sub_k in keyword_list:\n",
    "                    if sub_k not in v:\n",
    "                        match = False\n",
    "                        break\n",
    "                if match:\n",
    "                    if keyword in stats[k].keys():\n",
    "                        stats[k][keyword] += 1\n",
    "                    else:\n",
    "                        stats[k][keyword] = 1\n",
    "    return stats\n",
    "\n",
    "\n",
    "def get_keyword_statistics(command_structs, include_keyword=\"adverb\"):\n",
    "    stats = {}\n",
    "    for command in command_structs:\n",
    "        keyword = command[include_keyword]\n",
    "        if keyword in stats.keys():\n",
    "            stats[keyword] += 1\n",
    "        else:\n",
    "            stats[keyword] = 1\n",
    "    return stats\n",
    "\n",
    "def flatten_dictionary(\n",
    "    dictionary_in\n",
    "):\n",
    "    flat_dictionary = {}\n",
    "    for k, v in dictionary_in.items():\n",
    "        for kk, vv in v.items():\n",
    "            if kk not in flat_dictionary:\n",
    "                flat_dictionary[kk] = vv\n",
    "            else:\n",
    "                flat_dictionary[kk] += vv\n",
    "    return flat_dictionary\n",
    "\n",
    "def plot_dictionary(\n",
    "    dictionary_in,\n",
    "    y_label=\"Frequency\",\n",
    "    x_label=\"Conditions\",\n",
    "    title=\"Missing Title\",\n",
    "    save_file=None,\n",
    "    is_plot=False,\n",
    "    wandb=None,\n",
    "):\n",
    "    group_str = [k for k, _ in dictionary_in[0].items()]\n",
    "    if len(group_str) > 8:\n",
    "        rotate=90\n",
    "        fontsize=10\n",
    "    else:\n",
    "        rotate=45\n",
    "        fontsize=13\n",
    "    all_stats = []\n",
    "    for d in dictionary_in:\n",
    "        group_stats = [d[k] for k in group_str]\n",
    "        all_stats.append(group_stats)\n",
    "    all_stats = np.array(all_stats)\n",
    "    std = np.std(all_stats, axis=0)\n",
    "    mean = np.mean(all_stats, axis=0)\n",
    "\n",
    "    # input data\n",
    "    mean_values = mean\n",
    "    variance = std**2\n",
    "    bar_labels = group_str\n",
    "        \n",
    "    # plot bars\n",
    "    x_pos = list(range(len(bar_labels)))\n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    g = ax.bar(x_pos, mean_values, yerr=variance, align='center', alpha=0.5)\n",
    "\n",
    "    plt.grid()\n",
    "\n",
    "    # set height of the y-axis\n",
    "    max_y = max(zip(mean_values, variance)) # returns a tuple, here: (3, 5)\n",
    "    plt.ylim([0, (max_y[0] + max_y[1]) * 1.1])\n",
    "\n",
    "    # set axes labels and title\n",
    "    plt.ylabel(y_label)\n",
    "    \n",
    "    plt.xticks(x_pos, bar_labels)\n",
    "    plt.xticks(rotation = rotate, fontsize=fontsize)    \n",
    "    plt.yticks(rotation = 45)\n",
    "    plt.title(title, fontsize=10)\n",
    "    if mean_values[0] > 10000:\n",
    "        plt.ticklabel_format(axis='y', style='sci', scilimits=(4,4))\n",
    "    \n",
    "    if wandb != None:\n",
    "        # Let us also try to log this plot to wandb!\n",
    "        wandb.log({title: wandb.Image(fig)})\n",
    "\n",
    "    if save_file != None:\n",
    "        plt.savefig(save_file, dpi=100, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "    else:\n",
    "        if is_plot:\n",
    "            plt.show()\n",
    "    \n",
    "def get_command_struct_statistics(\n",
    "    command_structs, run_name=\"ReaSCAN-Awesome\", date=\"2021-05-06\", \n",
    "    split=\"demo\",\n",
    "    compositional_split=False,\n",
    "    n_sample=-1, n_runs=10,\n",
    "    output_dir=\"../../data-files/ReaSCAN-compositional_splits/\",\n",
    "    save_to_disk=True,\n",
    "    wandb=None\n",
    "):\n",
    "    statistics = OrderedDict({\n",
    "        \"run_name\": run_name,\n",
    "        \"date\": date,\n",
    "        \"splits\": split,\n",
    "        \"number_of_these_examples_seen_in_training\": -1 if not compositional_split else 0,\n",
    "        \"number_of_command_structs\": len(command_structs),\n",
    "    })\n",
    "    if n_sample == -1:\n",
    "        n_sample = len(command_structs)\n",
    "    # If we are downsampling, we need to do more runs as well!\n",
    "    random.shuffle(command_structs)\n",
    "    \n",
    "    patterns = set([])\n",
    "    for command_s in command_structs:\n",
    "        patterns.add(command_s[\"grammer_pattern\"])\n",
    "    statistics[\"command_patterns\"] = list(patterns)\n",
    "    \n",
    "    pattern_stats = get_keyword_statistics(command_structs, include_keyword=\"grammer_pattern\")\n",
    "    statistics[\"pattern_stats\"] = pattern_stats\n",
    "    \n",
    "    # verb\n",
    "    verb_stats = get_keyword_statistics(command_structs, include_keyword=\"verb\")\n",
    "    statistics[\"verb_stats\"] = verb_stats\n",
    "    plot_dictionary(\n",
    "        [verb_stats],\n",
    "        title=\"Verbs\",\n",
    "        save_file=os.path.join(output_dir, f\"verb_stats-{split}.png\"),\n",
    "        wandb=wandb,\n",
    "    )\n",
    "    \n",
    "    # adverb\n",
    "    adverb_stats = get_keyword_statistics(command_structs, include_keyword=\"adverb\")\n",
    "    # special handling for adverb for better readabilities\n",
    "    adverb_stats_rebuild = {}\n",
    "    for k, v in adverb_stats.items():\n",
    "        if k == \"\":\n",
    "            adverb_stats_rebuild[\"EMPTY\"] = v\n",
    "        else:\n",
    "            adverb_stats_rebuild[k] = v\n",
    "    statistics[\"adverb_stats\"] = adverb_stats_rebuild\n",
    "    plot_dictionary(\n",
    "        [adverb_stats_rebuild],\n",
    "        title=\"Adverbs\",\n",
    "        save_file=os.path.join(output_dir, f\"adverb_stats-{split}.png\"),\n",
    "        wandb=wandb,\n",
    "    )\n",
    "    \n",
    "    # relation\n",
    "    relation_stats = get_relation_statistics(command_structs)\n",
    "    if len(flatten_dictionary(relation_stats)) != 0:\n",
    "        statistics[\"relation_stats\"] = relation_stats\n",
    "        plot_dictionary(\n",
    "            [flatten_dictionary(relation_stats)],\n",
    "            title=\"Relation-Types\",\n",
    "            save_file=os.path.join(output_dir, f\"relation_type_stats-{split}.png\"),\n",
    "            wandb=wandb,\n",
    "        )\n",
    "    \n",
    "    # attribute\n",
    "    nouns = [\"circle\", \"cylinder\", \"square\", \"box\", \"object\"]\n",
    "    n_stats = get_attribute_statistics(command_structs, include_keywords=nouns)\n",
    "    statistics[\"shape_stats\"] = n_stats\n",
    "    plot_dictionary(\n",
    "        [flatten_dictionary(n_stats)],\n",
    "        title=\"Shapes\",\n",
    "        save_file=os.path.join(output_dir, f\"shape_stats-{split}.png\"),\n",
    "        wandb=wandb,\n",
    "    )\n",
    "    \n",
    "    color_adjectives = [\"red\", \"blue\", \"green\", \"yellow\"]\n",
    "    c_stats = get_attribute_statistics(command_structs, include_keywords=color_adjectives)\n",
    "    statistics[\"color_stats\"] = c_stats\n",
    "    if len(flatten_dictionary(c_stats)) != 0:\n",
    "        plot_dictionary(\n",
    "            [flatten_dictionary(c_stats)],\n",
    "            title=\"Colors\",\n",
    "            save_file=os.path.join(output_dir, f\"color_stats-{split}.png\"),\n",
    "            wandb=wandb,\n",
    "        )\n",
    "\n",
    "    size_adjectives = [\"big\", \"small\"]\n",
    "    s_stats = get_attribute_statistics(command_structs, include_keywords=size_adjectives)\n",
    "    if len(flatten_dictionary(s_stats)) != 0:\n",
    "        statistics[\"size_stats\"] = s_stats\n",
    "        plot_dictionary(\n",
    "            [flatten_dictionary(s_stats)],\n",
    "            title=\"Sizes\",\n",
    "            save_file=os.path.join(output_dir, f\"size_stats-{split}.png\"),\n",
    "            wandb=wandb,\n",
    "        )\n",
    "    \n",
    "    # second order attribute\n",
    "    color_adjectives = [\"red\", \"blue\", \"green\", \"yellow\"]\n",
    "    nouns = [\"circle\", \"cylinder\", \"square\", \"box\", \"object\"]\n",
    "    c_n_p = product(color_adjectives, nouns)\n",
    "    include_keywords = [\" \".join(c_n) for c_n in c_n_p]\n",
    "    c_n_stats = get_attribute_statistics(command_structs, include_keywords=include_keywords)\n",
    "    statistics[\"color_and_shape_stats\"] = c_n_stats\n",
    "    if len(flatten_dictionary(c_n_stats)) != 0:\n",
    "        plot_dictionary(\n",
    "            [flatten_dictionary(c_n_stats)],\n",
    "            title=\"Colors-Shapes\",\n",
    "            save_file=os.path.join(output_dir, f\"color+shape_stats-{split}.png\"),\n",
    "            wandb=wandb,\n",
    "        )\n",
    "\n",
    "    size_adjectives = [\"big\", \"small\"]\n",
    "    nouns = [\"circle\", \"cylinder\", \"square\", \"box\", \"object\"]\n",
    "    s_n_p = product(size_adjectives, nouns)\n",
    "    include_keywords = [\" \".join(s_n) for s_n in s_n_p]\n",
    "    s_n_stats = get_attribute_statistics(command_structs, include_keywords=include_keywords)\n",
    "    statistics[\"size_and_shape_stats\"] = s_n_stats\n",
    "    if len(flatten_dictionary(s_n_stats)) != 0:\n",
    "        plot_dictionary(\n",
    "            [flatten_dictionary(s_n_stats)],\n",
    "            title=\"Sizes-Shapes\",\n",
    "            save_file=os.path.join(output_dir, f\"size+shape_stats-{split}.png\"),\n",
    "            wandb=wandb,\n",
    "        )\n",
    "    \n",
    "    # third order attribute\n",
    "    size_adjectives = [\"big\", \"small\"]\n",
    "    color_adjectives = [\"red\", \"blue\", \"green\", \"yellow\"]\n",
    "    nouns = [\"circle\", \"cylinder\", \"square\", \"box\", \"object\"]\n",
    "    all_p = product(size_adjectives, color_adjectives, nouns)\n",
    "    include_keywords = [\" \".join(a) for a in all_p]\n",
    "    all_stats = get_attribute_statistics(command_structs, include_keywords=include_keywords)\n",
    "    statistics[\"size_and_color_and_shape_stats\"] = all_stats\n",
    "    \n",
    "    if save_to_disk:\n",
    "        import yaml\n",
    "        with open(os.path.join(output_dir, f\"command_struct_only_stats-{split}.yml\"), 'w') as yaml_file:\n",
    "            yaml.dump(statistics, yaml_file, default_flow_style=False)\n",
    "    \n",
    "    return statistics\n",
    "\n",
    "def arg_parse():\n",
    "    \n",
    "    # This is a single loop to generate the dataset.\n",
    "    n_processes = 1\n",
    "    mode = \"all\"\n",
    "    n_command_struct = 10000\n",
    "    grid_size = 6\n",
    "    n_object_max = 10\n",
    "    seed = 42\n",
    "    date = \"2021-05-07\"\n",
    "    per_command_world_retry_max = 200\n",
    "    per_command_world_target_count = 10 # for each command, we target to have 50 shapeWorld!\n",
    "    resumed_from_file_path = \"\"\n",
    "    is_tensorboard = False\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='ReaSCAN argparse.')\n",
    "    # Experiment management:\n",
    "    parser.add_argument('--n_processes', type=int, default=1,\n",
    "                        help='Number of process used to generate the dataset.')\n",
    "    parser.add_argument('--index_start', type=int, default=-1,\n",
    "                        help='Number of command sampled from the command population.')\n",
    "    parser.add_argument('--index_end', type=int, default=-1,\n",
    "                        help='Number of command sampled from the command population.')\n",
    "\n",
    "    parser.add_argument('--mode', type=str, default=\"all\",\n",
    "                        help='mode')\n",
    "    parser.add_argument('--n_command_struct', type=int, default=10000,\n",
    "                        help='Number of command sampled from the command population.')\n",
    "    parser.add_argument('--grid_size', type=int, default=6,\n",
    "                        help='Grid size of the world.')\n",
    "    parser.add_argument('--n_object_max', type=int, default=10,\n",
    "                        help='Number of object at max in the shapeWorld (Note that you may still have more than this number!).')\n",
    "    parser.add_argument('--seed', type=int, default=42,\n",
    "                        help='Random seed.')\n",
    "    parser.add_argument('--date', type=str,\n",
    "                        help='date')\n",
    "    parser.add_argument('--per_command_world_retry_max', type=int, default=200,\n",
    "                        help='How many times you can retry for each world generation.')\n",
    "    parser.add_argument('--per_command_world_target_count', type=int, default=50,\n",
    "                        help='The targeted number of world to have per command.')\n",
    "    parser.add_argument(\"--is_tensorboard\",\n",
    "                        default=False,\n",
    "                        action='store_true',\n",
    "                        help=\"Whether to use tensorboard.\")\n",
    "    \n",
    "    parser.add_argument(\"--include_relation_distractor\",\n",
    "                        default=False,\n",
    "                        action='store_true',\n",
    "                        help=\"Whether to use tensorboard.\")\n",
    "    parser.add_argument(\"--include_attribute_distractor\",\n",
    "                        default=False,\n",
    "                        action='store_true',\n",
    "                        help=\"Whether to use tensorboard.\")\n",
    "    parser.add_argument(\"--include_isomorphism_distractor\",\n",
    "                        default=False,\n",
    "                        action='store_true',\n",
    "                        help=\"Whether to use tensorboard.\")\n",
    "    parser.add_argument(\"--include_random_distractor\",\n",
    "                        default=False,\n",
    "                        action='store_true',\n",
    "                        help=\"Whether to use tensorboard.\")\n",
    "    parser.add_argument('--full_relation_probability', type=float, default=1.0,\n",
    "                        help='Probability of including full relation distractors.')\n",
    "    \n",
    "    parser.add_argument('--save_interal', type=int, default=200,\n",
    "                        help='Saving intervel in command count.')\n",
    "    \n",
    "    \n",
    "    parser.add_argument('--command_pattern', type=str, default=\"p3\",\n",
    "                        help='What pattern to use, currently, we support p1-p4.')\n",
    "    \n",
    "    parser.add_argument('--resumed_from_file_path', type=str, default=\"\",\n",
    "                        help='Whether to resume for this file.')\n",
    "    parser.add_argument('--output_dir', type=str, default=\"../../data-files/ReaSCAN-compositional_splits/\",\n",
    "                        help='Whether to resume for this file.')\n",
    "\n",
    "    parser.set_defaults(\n",
    "        # Exp management:\n",
    "        n_processes=1,\n",
    "        mode=\"all\",\n",
    "        n_command_struct=10000,\n",
    "        grid_size=6,\n",
    "        n_object_max=10,\n",
    "        seed=42,\n",
    "        date=\"2021-05-07\",\n",
    "        per_command_world_retry_max=200,\n",
    "        per_command_world_target_count=50,\n",
    "        resumed_from_file_path=\"\",\n",
    "        is_tensorboard=False,\n",
    "        output_dir=\"../../data-files/ReaSCAN-compositional_splits/\",\n",
    "    )\n",
    "    try:\n",
    "        get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "        args = parser.parse_args([])\n",
    "    except:\n",
    "        args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "def example_classifier(\n",
    "    task_info,\n",
    "    mode=\"demo\",\n",
    "    default_split_prob={\n",
    "        \"train\": 0.9, \n",
    "        \"dev\": 0.01,\n",
    "        \"test\": 0.09,\n",
    "    },\n",
    "):\n",
    "    \"\"\"\n",
    "    This will return the split this data belongs to.\n",
    "    \"\"\"\n",
    "    if mode == \"demo\" or mode == \"all\":\n",
    "        if random.random() < default_split_prob[\"train\"]:\n",
    "            return \"train\"\n",
    "        else:\n",
    "            if random.random() < 0.9:\n",
    "                return \"test\"\n",
    "            else:\n",
    "                return \"dev\"\n",
    "    else:\n",
    "        # We need to add here logics to determine\n",
    "        # compositional splits!\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some tips:\n",
    "# Do not debug in this file, you can simply copy the questionable struct\n",
    "# to the lightweight demo file, and you can debug there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Loading arguments\n",
    "    args = arg_parse()\n",
    "    try:\n",
    "#         get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "#         # Experiment management:\n",
    "#         args.n_processes=1\n",
    "#         args.mode=\"demo\"\n",
    "#         args.n_command_struct=20\n",
    "#         args.grid_size=6\n",
    "#         args.n_object_max=10\n",
    "#         args.seed=42\n",
    "#         args.date=\"2021-05-07\"\n",
    "#         args.per_command_world_retry_max=20\n",
    "#         args.per_command_world_target_count=3\n",
    "#         args.resumed_from_file_path=\"\"\n",
    "#         args.is_tensorboard=True # Let us try this!\n",
    "#         args.output_dir=\"../../data-files/ReaSCAN-demo/\"\n",
    "#         is_jupyter = True\n",
    "        \n",
    "        get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "        # Experiment management:\n",
    "        args.n_processes=1\n",
    "        args.mode=\"train\"\n",
    "        args.n_command_struct=675*5\n",
    "        args.grid_size=6\n",
    "        args.n_object_max=10\n",
    "        args.seed=42\n",
    "        args.save_interal = 200\n",
    "        args.date=\"2021-05-30\"\n",
    "        args.per_command_world_retry_max=1000\n",
    "        args.per_command_world_target_count=180\n",
    "        args.resumed_from_file_path=\"\"\n",
    "        args.is_tensorboard=True # Let us try this!\n",
    "        args.output_dir=\"../../data-files/ReaSCAN-compositional-p3-full-relation/\"\n",
    "        is_jupyter = True\n",
    "        \n",
    "        args.index_start = -1\n",
    "        args.index_end = -1\n",
    "    except:\n",
    "        is_jupyter = False\n",
    "    \n",
    "    loading_p1 = True if args.command_pattern == \"p1\" else False\n",
    "    p1_exhaustive_verb_adverb = False\n",
    "    loading_p2 = True if args.command_pattern == \"p2\" else False\n",
    "    loading_p3 = True if args.command_pattern == \"p3\" else False\n",
    "    loading_p4 = True if args.command_pattern == \"p4\" else False\n",
    "\n",
    "    save_command_stats = False\n",
    "    save_at_interval = True\n",
    "    save_interal = args.save_interal\n",
    "\n",
    "    # TODO: add these to args.\n",
    "    logging_interval = 1000\n",
    "    \n",
    "    # Create output directory if not exists.\n",
    "    pathlib.Path(args.output_dir).mkdir(parents=True, exist_ok=True) \n",
    "    \n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO, \n",
    "        format='%(asctime)s %(levelname)-8s %(message)s', \n",
    "        datefmt='%a, %d %b %Y %H:%M:%S', \n",
    "        filename=os.path.join(args.output_dir, \"generator.log\"),\n",
    "    )\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.getLogger().addHandler(logging.StreamHandler(os.sys.stdout))\n",
    "    \n",
    "    logger.info(\"Generating ReaSCAN with following parameters: \")\n",
    "    logger.info(args)\n",
    "    \n",
    "    # This is a single loop to generate the dataset.\n",
    "    n_processes = args.n_processes\n",
    "    mode = args.mode\n",
    "    n_command_struct = args.n_command_struct\n",
    "    grid_size = args.grid_size\n",
    "    n_object_max = args.n_object_max\n",
    "    seed = args.seed\n",
    "    date = args.date\n",
    "    per_command_world_retry_max = args.per_command_world_retry_max\n",
    "    per_command_world_target_count = args.per_command_world_target_count # for each command, we target to have 50 shapeWorld!\n",
    "    resumed_from_file_path = args.resumed_from_file_path\n",
    "    output_dir = args.output_dir\n",
    "    is_tensorboard = args.is_tensorboard\n",
    "    \n",
    "    if is_tensorboard:\n",
    "        logger.warning(\"Enabling wandb for tensorboard logging...\")\n",
    "        import wandb\n",
    "        run = wandb.init(project=\"ReaSCAN\", entity=\"wuzhengx\")\n",
    "        run_name = wandb.run.name\n",
    "        wandb.config.update(args)\n",
    "    else:\n",
    "        wandb = None\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # We also need something to generate generalization\n",
    "    # splits!\n",
    "    params = {\n",
    "        \"n_processes\": n_processes,\n",
    "        \"mode\": mode,\n",
    "        \"n_command_struct\": n_command_struct,\n",
    "        \"grid_size\": grid_size,\n",
    "        \"n_object_max\": n_object_max,\n",
    "        \"seed\": seed,\n",
    "        \"per_command_world_retry_max\": per_command_world_retry_max,\n",
    "        \"per_command_world_target_count\": per_command_world_target_count,\n",
    "    }\n",
    "    \n",
    "    if mode == \"all\" or mode == \"demo\" or mode == \"train\":\n",
    "        # Meaning we are generating the random ReaSCAN train + dev + test splits!\n",
    "        logger.warning(f\"You are generating data for {mode} splits only!\")\n",
    "        split_percentage = {\n",
    "            \"train\": 0.9, \n",
    "        }\n",
    "    elif mode == \"all,noval_1,noval_2,noval_3,noval_4\":\n",
    "        # here we need to define how to check for noval_*\n",
    "        pass\n",
    "    elif mode == \"compositional\":\n",
    "        # Meaning we are generating the random ReaSCAN train + dev + test splits!\n",
    "        logger.warning(\"You are generating data for all compositional splits!\")\n",
    "    elif mode == \"\":\n",
    "        pass # Not implemented!\n",
    "        \n",
    "    # Using the full vocabulary.\n",
    "    intransitive_verbs = [\"walk\"]\n",
    "    transitive_verbs = [\"push\", \"pull\"]\n",
    "    adverbs = [\"while zigzagging\", \"while spinning\", \"cautiously\", \"hesitantly\"]\n",
    "    nouns = [\"circle\", \"cylinder\", \"square\", \"box\"]\n",
    "    color_adjectives = [\"red\", \"blue\", \"green\", \"yellow\"]\n",
    "    size_adjectives = [\"big\", \"small\"]\n",
    "    relative_pronouns = [\"that is\"]\n",
    "    relation_clauses = [\"in the same row as\", \n",
    "                        \"in the same column as\", \n",
    "                        \"in the same color as\", \n",
    "                        \"in the same shape as\", \n",
    "                        \"in the same size as\",\n",
    "                        \"inside of\"]\n",
    "    vocabulary = Vocabulary.initialize(intransitive_verbs=intransitive_verbs,\n",
    "                                       transitive_verbs=transitive_verbs, adverbs=adverbs, nouns=nouns,\n",
    "                                       color_adjectives=color_adjectives,\n",
    "                                       size_adjectives=size_adjectives, \n",
    "                                       relative_pronouns=relative_pronouns, \n",
    "                                       relation_clauses=relation_clauses)\n",
    "    \n",
    "    # test out the object vocab\n",
    "    min_object_size = 1\n",
    "    max_object_size = 4\n",
    "    object_vocabulary = ObjectVocabulary(shapes=vocabulary.get_semantic_shapes(),\n",
    "                                         colors=vocabulary.get_semantic_colors(),\n",
    "                                         min_size=min_object_size, max_size=max_object_size)\n",
    "    \n",
    "    # Generating all the core command structs.\n",
    "    grammer = Grammer(vocabulary)\n",
    "    \n",
    "    # Bootup our simulator.\n",
    "    simulator = Simulator(\n",
    "        object_vocabulary, vocabulary, \n",
    "        grid_size=grid_size, \n",
    "        n_object_max=n_object_max,\n",
    "    )\n",
    "    \n",
    "    command_structs = []\n",
    "    logger.info(\"Finished loading required modules...\")\n",
    "    # Sampling all the possible command score structs.\n",
    "    \n",
    "    if loading_p4:\n",
    "        # Currently, we hard-code the pattern!\n",
    "        grammer_pattern = '$OBJ_0 ^ $OBJ_1 & $OBJ_2 & $OBJ_3'\n",
    "        logger.info(f\"Including pattern:= {grammer_pattern}...\")\n",
    "        # Sampling relations\n",
    "        relations = grammer.sample_object_relation_grammer(\n",
    "            '$OBJ_0', \n",
    "            grammer.build_dependency_graph(grammer_pattern))\n",
    "        for relation in relations:\n",
    "            obj_pattern_map = relation[0]\n",
    "            rel_map = relation[1]\n",
    "            grammer_bindings = grammer.grounding_grammer_with_vocabulary(grammer_pattern, obj_pattern_map, rel_map)\n",
    "            for obj_map in grammer_bindings:\n",
    "                # here, we also sample the verb and adverb bindings!\n",
    "                adverb_enhance_list = vocabulary.get_adverbs()\n",
    "                adverb_enhance_list += [\"\"]\n",
    "                command_struct = {\n",
    "                    \"obj_pattern_map\" : obj_pattern_map,\n",
    "                    \"rel_map\" : rel_map,\n",
    "                    \"obj_map\" : obj_map,\n",
    "                    \"grammer_pattern\" : grammer_pattern,\n",
    "                    \"adverb\" : random.choice(adverb_enhance_list),\n",
    "                    \"verb\" : random.choice(vocabulary.get_transitive_verbs() + vocabulary.get_intransitive_verbs()),\n",
    "                }\n",
    "                command_structs += [command_struct]\n",
    "                \n",
    "    if loading_p3:\n",
    "        # Currently, we hard-code the pattern!\n",
    "        grammer_pattern = '$OBJ_0 ^ $OBJ_1 & $OBJ_2'\n",
    "        logger.info(f\"Including pattern:= {grammer_pattern}...\")\n",
    "        # Sampling relations\n",
    "        relations = grammer.sample_object_relation_grammer(\n",
    "            '$OBJ_0', \n",
    "            grammer.build_dependency_graph(grammer_pattern))\n",
    "        for relation in relations:\n",
    "            obj_pattern_map = relation[0]\n",
    "            rel_map = relation[1]\n",
    "            grammer_bindings = grammer.grounding_grammer_with_vocabulary(grammer_pattern, obj_pattern_map, rel_map)\n",
    "            for obj_map in grammer_bindings:\n",
    "                # here, we also sample the verb and adverb bindings!\n",
    "                adverb_enhance_list = vocabulary.get_adverbs()\n",
    "                adverb_enhance_list += [\"\"]\n",
    "                command_struct = {\n",
    "                    \"obj_pattern_map\" : obj_pattern_map,\n",
    "                    \"rel_map\" : rel_map,\n",
    "                    \"obj_map\" : obj_map,\n",
    "                    \"grammer_pattern\" : grammer_pattern,\n",
    "                    \"adverb\" : random.choice(adverb_enhance_list),\n",
    "                    \"verb\" : random.choice(vocabulary.get_transitive_verbs() + vocabulary.get_intransitive_verbs()),\n",
    "                }\n",
    "                command_structs += [command_struct]\n",
    "    \n",
    "    if loading_p2:\n",
    "        grammer_pattern = '$OBJ_0 ^ $OBJ_1'\n",
    "        logger.info(f\"Including pattern:= {grammer_pattern}...\")\n",
    "        # Sampling relations\n",
    "        relations = grammer.sample_object_relation_grammer(\n",
    "            '$OBJ_0', \n",
    "            grammer.build_dependency_graph(grammer_pattern))\n",
    "        for relation in relations:\n",
    "            obj_pattern_map = relation[0]\n",
    "            rel_map = relation[1]\n",
    "            grammer_bindings = grammer.grounding_grammer_with_vocabulary(grammer_pattern, obj_pattern_map, rel_map)\n",
    "            for obj_map in grammer_bindings:\n",
    "                # here, we also sample the verb and adverb bindings!\n",
    "                adverb_enhance_list = vocabulary.get_adverbs()\n",
    "                adverb_enhance_list += [\"\"]\n",
    "                command_struct = {\n",
    "                    \"obj_pattern_map\" : obj_pattern_map,\n",
    "                    \"rel_map\" : rel_map,\n",
    "                    \"obj_map\" : obj_map,\n",
    "                    \"grammer_pattern\" : grammer_pattern,\n",
    "                    \"adverb\" : random.choice(adverb_enhance_list),\n",
    "                    \"verb\" : random.choice(vocabulary.get_transitive_verbs() + vocabulary.get_intransitive_verbs()),\n",
    "                }\n",
    "                command_structs += [command_struct]\n",
    "    \n",
    "    if loading_p1:\n",
    "        p1_exhaustive_verb_adverb = True\n",
    "        # for gSCAN command, we don't need to undersample, they are small!\n",
    "        grammer_pattern = '$OBJ_0'\n",
    "        logger.info(f\"Including pattern:= {grammer_pattern}...\")\n",
    "        # Sampling relations\n",
    "        relations = grammer.sample_object_relation_grammer(\n",
    "            '$OBJ_0', \n",
    "            grammer.build_dependency_graph(grammer_pattern))\n",
    "        for relation in relations:\n",
    "            obj_pattern_map = relation[0]\n",
    "            rel_map = relation[1]\n",
    "            grammer_bindings = grammer.grounding_grammer_with_vocabulary(grammer_pattern, obj_pattern_map, rel_map)\n",
    "            for obj_map in grammer_bindings:\n",
    "                if p1_exhaustive_verb_adverb:\n",
    "                    for adverb in vocabulary.get_adverbs() + [\"\"]:\n",
    "                        for verb in vocabulary.get_transitive_verbs() + vocabulary.get_intransitive_verbs():\n",
    "                            # here, we also sample the verb and adverb bindings!\n",
    "                            command_struct = {\n",
    "                                \"obj_pattern_map\" : obj_pattern_map,\n",
    "                                \"rel_map\" : rel_map,\n",
    "                                \"obj_map\" : obj_map,\n",
    "                                \"grammer_pattern\" : grammer_pattern,\n",
    "                                \"adverb\" : adverb,\n",
    "                                \"verb\" : verb,\n",
    "                            }\n",
    "                            command_structs += [command_struct]\n",
    "            \n",
    "    # We only sample these command!\n",
    "    \"\"\"\n",
    "    WARNING: beaware that not all command struct can\n",
    "    be sampled for world-command pair! They may or\n",
    "    may not fail.\n",
    "    \"\"\"\n",
    "    under_sample = True\n",
    "    if under_sample:\n",
    "        sampled_command_struct = []\n",
    "        random.shuffle(command_structs)\n",
    "        if n_command_struct != -1:\n",
    "            sampled_command_struct = command_structs[:n_command_struct]\n",
    "        if args.index_start == -1 or args.index_end == -1:\n",
    "            pass\n",
    "        else:\n",
    "            # we only look at one shard! this is for multiprocess\n",
    "            logger.info(f\"WARNING: contine with sharding: start at {args.index_start}; end at {args.index_end}\")\n",
    "            sampled_command_struct = command_structs[args.index_start:args.index_end]\n",
    "        logger.info(f\"Sampled {len(sampled_command_struct)} from {len(command_structs)} core command structs for pattern={grammer_pattern}.\")\n",
    "            \n",
    "    logger.info(f\"Finished sampling core command structs with total {len(sampled_command_struct)}...\")\n",
    "    \n",
    "    command_struct_file_path = os.path.join(args.output_dir, f\"command_struct-{args.mode}.txt\")\n",
    "    formatted_sampled_command_struct = []\n",
    "    for command_struct in sampled_command_struct:\n",
    "        formatted_command_struct = {\n",
    "            \"obj_pattern_map\" : command_struct[\"obj_pattern_map\"],\n",
    "            \"rel_map\" : [(k, v) for k, v in command_struct[\"rel_map\"].items()],\n",
    "            \"obj_map\" : command_struct[\"obj_map\"],\n",
    "            \"grammer_pattern\" : command_struct[\"grammer_pattern\"],\n",
    "            \"adverb\" : command_struct[\"adverb\"],\n",
    "            \"verb\" : command_struct[\"verb\"],\n",
    "        }\n",
    "        formatted_sampled_command_struct += [formatted_command_struct]\n",
    "    # dump to the disk.\n",
    "    with open(command_struct_file_path, \"w\") as fd:\n",
    "        json.dump(formatted_sampled_command_struct, fd, indent=4)\n",
    "    logger.info(f\"Saved command struct to {command_struct_file_path} for later use...\")\n",
    "                    \n",
    "    # print out quick stats on how many command per pattern!\n",
    "    per_pattern_command_count = {}\n",
    "    for command_struct in sampled_command_struct:\n",
    "        grammer_pattern = command_struct[\"grammer_pattern\"]\n",
    "        if grammer_pattern in per_pattern_command_count.keys():\n",
    "            per_pattern_command_count[grammer_pattern] += 1\n",
    "        else:\n",
    "            per_pattern_command_count[grammer_pattern] = 1\n",
    "    logger.info(f\"Counts per command pattern: \")\n",
    "    logger.info(per_pattern_command_count)\n",
    "\n",
    "    # From the struct, let us sample shape world.\n",
    "    \"\"\"\n",
    "    We just need a couple more steps beyond this point:\n",
    "    (1) Sample a world\n",
    "    (2) Making sure it is valid\n",
    "    (3) Construct the command, providing determiners\n",
    "    (4) Generate action sequences to the target\n",
    "    (5) Get all the action related metadata as gSCAN\n",
    "    (6) Save it to per command example\n",
    "    \"\"\"\n",
    "    \n",
    "    # We need a way to index the sampled command.\n",
    "    sampled_command_struct_indexed = OrderedDict({})\n",
    "    global_command_struct_index = 0\n",
    "    for command_struct in sampled_command_struct:\n",
    "        sampled_command_struct_indexed[global_command_struct_index] = command_struct\n",
    "        global_command_struct_index += 1\n",
    "    \n",
    "    root = \"$OBJ_0\"\n",
    "    per_command_world_counts = OrderedDict({})\n",
    "    if mode == \"demo\" or mode == \"all\" or mode == \"train\":\n",
    "        created_examples_by_splits = OrderedDict({\n",
    "            \"train\" : [],\n",
    "        })\n",
    "    else:\n",
    "        pass\n",
    "    shaperized_command_struct = []\n",
    "    per_command_world_unique_check = OrderedDict({})\n",
    "    \n",
    "    # Some global control for data quality control.\n",
    "    global_step = 0\n",
    "    success_step = 0\n",
    "    \n",
    "    # Distractor info logs.\n",
    "    d_full_relation_count = 0\n",
    "    d_relation_count = 0\n",
    "    d_attribute_count = 0\n",
    "    d_iso_count = 0\n",
    "    d_random_count = 0\n",
    "    \n",
    "    logger.info(f\"Started to generate the dataset...\")\n",
    "    for command_struct_index, command_struct in sampled_command_struct_indexed.items():\n",
    "        logger.info(f\"Generating for command struct (seed={seed}): {command_struct_index+1}/{len(sampled_command_struct_indexed)}...\")\n",
    "        per_command_world_counts[command_struct_index] = 0 # 0 world for each command in the beginning!\n",
    "        per_command_world_unique_check[command_struct_index] = set([])\n",
    "        obj_pattern_map = command_struct[\"obj_pattern_map\"]\n",
    "        rel_map = command_struct[\"rel_map\"]\n",
    "        obj_map = command_struct[\"obj_map\"]\n",
    "        grammer_pattern = command_struct[\"grammer_pattern\"]\n",
    "        verb = command_struct[\"verb\"]\n",
    "        adverb = command_struct[\"adverb\"]\n",
    "        # This is the target world number generated for this command\n",
    "        for n_world_try in range(per_command_world_target_count):\n",
    "            # How many time we need to retry before we give up?\n",
    "            at_least_success = False\n",
    "            for n_retry in range(per_command_world_retry_max):\n",
    "                global_step += 1\n",
    "                if success_step == 0:\n",
    "                    denom = 1\n",
    "                else:\n",
    "                    denom = success_step\n",
    "                d_full_relation_ratio = 1.0*d_full_relation_count/denom\n",
    "                d_relation_ratio = 1.0*d_relation_count/denom\n",
    "                d_attribute_ratio = 1.0*d_attribute_count/denom\n",
    "                d_iso_ratio = 1.0*d_iso_count/denom\n",
    "                d_random_ratio = 1.0*d_random_count/denom\n",
    "                global_success_ratio = 1.0*success_step/global_step\n",
    "                # logging some very useful information to wandb if avaliable!\n",
    "                if is_tensorboard:\n",
    "                    if (global_step%logging_interval) == 0:\n",
    "                        wandb.log({'global_success_ratio': global_success_ratio, 'global_step': global_step})\n",
    "                        wandb.log({'current_example_count': success_step, 'global_step': global_step})\n",
    "                        wandb.log({'d_full_relation_ratio': d_full_relation_ratio, 'global_step': global_step})\n",
    "                        wandb.log({'d_relation_ratio': d_relation_ratio, 'global_step': global_step})\n",
    "                        wandb.log({'d_attribute_ratio': d_attribute_ratio, 'global_step': global_step})\n",
    "                        wandb.log({'d_iso_ratio': d_iso_ratio, 'global_step': global_step})\n",
    "                        wandb.log({'d_random_ratio': d_random_ratio, 'global_step': global_step})  \n",
    "                else:\n",
    "                    if (global_step%(logging_interval*10)) == 0:\n",
    "                        logger.info({'global_success_ratio': global_success_ratio, 'global_step': global_step})\n",
    "                        logger.info({'current_example_count': success_step, 'global_step': global_step})\n",
    "                        logger.info({'d_full_relation_ratio': d_full_relation_ratio, 'global_step': global_step})\n",
    "                        logger.info({'d_relation_ratio': d_relation_ratio, 'global_step': global_step})\n",
    "                        logger.info({'d_attribute_ratio': d_attribute_ratio, 'global_step': global_step})\n",
    "                        logger.info({'d_iso_ratio': d_iso_ratio, 'global_step': global_step})\n",
    "                        logger.info({'d_random_ratio': d_random_ratio, 'global_step': global_step})\n",
    "                    \n",
    "                if mode == \"demo\":\n",
    "                    sampled_world = simulator.sample_situations_from_grounded_grammer(\n",
    "                        copy.deepcopy(grammer_pattern), \n",
    "                        copy.deepcopy(obj_pattern_map), \n",
    "                        copy.deepcopy(rel_map), \n",
    "                        copy.deepcopy(obj_map),\n",
    "                        is_plot=False,\n",
    "                        include_relation_distractor=args.include_relation_distractor, \n",
    "                        include_attribute_distractor=args.include_attribute_distractor, \n",
    "                        include_isomorphism_distractor=args.include_isomorphism_distractor, \n",
    "                        include_random_distractor=args.include_random_distractor,\n",
    "                        full_relation_probability=args.full_relation_probability,\n",
    "                        debug=False\n",
    "                    ) # This is the minimum settings! You need to turn on attribute always!\n",
    "                else:\n",
    "                    # Sample a shapeWorld!\n",
    "                    sampled_world = simulator.sample_situations_from_grounded_grammer(\n",
    "                        copy.deepcopy(grammer_pattern), \n",
    "                        copy.deepcopy(obj_pattern_map), \n",
    "                        copy.deepcopy(rel_map), \n",
    "                        copy.deepcopy(obj_map),\n",
    "                        is_plot=False,\n",
    "                        include_relation_distractor=args.include_relation_distractor, \n",
    "                        include_attribute_distractor=args.include_attribute_distractor, \n",
    "                        include_isomorphism_distractor=args.include_isomorphism_distractor, \n",
    "                        include_random_distractor=args.include_random_distractor,\n",
    "                        full_relation_probability=args.full_relation_probability, # ReaSCAN Special: 15 distractors!\n",
    "                        debug=False\n",
    "                    )\n",
    "\n",
    "                # Validate the world is valid!\n",
    "                graph = ReaSCANGraph(\n",
    "                    objects=sampled_world[\"obj_map\"], \n",
    "                    object_patterns=sampled_world[\"obj_pattern_map\"], \n",
    "                    vocabulary=vocabulary,\n",
    "                    positions=sampled_world[\"pos_map\"], \n",
    "                    referred_object=sampled_world[\"referred_obj\"],\n",
    "                    debug=False\n",
    "                )\n",
    "                \n",
    "                pattern_graph = ReaSCANGraph(\n",
    "                    objects=obj_map, \n",
    "                    object_patterns=None,\n",
    "                    vocabulary=vocabulary,\n",
    "                    relations=rel_map, \n",
    "                    referred_object='$OBJ_0', \n",
    "                    debug=False\n",
    "                )\n",
    "                \n",
    "                potential_referent_target = graph.find_referred_object_super_fast(\n",
    "                    pattern_graph, referred_object='$OBJ_0', \n",
    "                    debug=False\n",
    "                )\n",
    "\n",
    "                # Save the result if the world is valid!\n",
    "                \n",
    "                # This may be to strict, but it ensures 100% correct!\n",
    "                if len(potential_referent_target) == 1 and '$OBJ_0' in potential_referent_target:\n",
    "                    # A quick world repeat check!\n",
    "                    hash_world_str = hashlib.md5(str(sampled_world[\"situation\"].to_representation()).encode('utf-8')).hexdigest()\n",
    "                    if hash_world_str not in per_command_world_unique_check[command_struct_index]:\n",
    "                        per_command_world_unique_check[command_struct_index].add(hash_world_str)\n",
    "                    else:\n",
    "                        continue # This is highly unlikely, but just to prevent!\n",
    "                    \n",
    "                    # Form the command with grounded determiners!\n",
    "                    obj_determiner_map = graph.find_determiners(\n",
    "                        pattern_graph, \n",
    "                        referred_object='$OBJ_0', \n",
    "                        debug=False,\n",
    "                    )\n",
    "                    \n",
    "                    # we don't check this for P1 and P2?\n",
    "                    \n",
    "#                     valid_determiner = True\n",
    "#                     for k, v in obj_determiner_map.items():\n",
    "#                         if k != '$OBJ_0':\n",
    "#                             if v != \"a\":\n",
    "#                                 valid_determiner = False\n",
    "#                                 break\n",
    "#                     if not valid_determiner:\n",
    "#                         continue # we should abort and resample!\n",
    "                    \n",
    "                    at_least_success = True\n",
    "                    success_step += 1\n",
    "                    \n",
    "                    command_str = grammer.repre_str_command(\n",
    "                        grammer_pattern, rel_map, obj_map, \n",
    "                        obj_determiner_map, \n",
    "                        verb,\n",
    "                        adverb,\n",
    "                    )\n",
    "\n",
    "                    # Form the golden label for the action list!\n",
    "                    is_transitive = False\n",
    "                    if verb in simulator.vocabulary.get_transitive_verbs():\n",
    "                        is_transitive = True\n",
    "                    # Direct walk.\n",
    "                    action = \"walk\" # this is definit!\n",
    "                    primitive_command = simulator.vocabulary.translate_word(action)\n",
    "                    target_position = sampled_world[\"situation\"].target_object.position\n",
    "                    simulator._world.go_to_position(\n",
    "                        position=target_position, manner=adverb, \n",
    "                        primitive_command=primitive_command\n",
    "                    )\n",
    "                    # Object actions.\n",
    "                    if is_transitive:\n",
    "                        semantic_action = simulator.vocabulary.translate_word(verb)\n",
    "                        simulator._world.move_object_to_wall(action=semantic_action, manner=adverb)\n",
    "                    target_commands, _ = simulator._world.get_current_observations()\n",
    "                    \n",
    "                    has_relation_distractor = False\n",
    "                    full_relation_distractor = True\n",
    "                    for rel_bool in sampled_world[\"distractor_switch_map\"][\"relation\"]:\n",
    "                        if rel_bool:\n",
    "                            has_relation_distractor = True\n",
    "                        else:\n",
    "                            full_relation_distractor = False\n",
    "                    \n",
    "                    # Save all relevant information for a task.\n",
    "                    task_struct = OrderedDict({\n",
    "                        \"command\": \",\".join(command_str.split(\" \")),\n",
    "                        \"grammer_pattern\": grammer_pattern,\n",
    "                        \"meaning\": \",\".join(command_str.split(\" \")),\n",
    "                        \"derivation\": grammer_pattern,\n",
    "                        \"situation\": sampled_world[\"situation\"].to_representation(),\n",
    "                        \"target_commands\": \",\".join(target_commands),\n",
    "                        \"verb_in_command\": verb,\n",
    "                        \"adverb_in_command\": adverb,\n",
    "                        \"referred_target\": obj_map[\"$OBJ_0\"],\n",
    "                        \"object_pattern_map\": obj_pattern_map,\n",
    "                        \"relation_map\": [(k, v) for k, v in rel_map.items()],\n",
    "                        \"object_expression\": obj_map,\n",
    "                        \"n_object\": len(sampled_world[\"obj_map\"]),\n",
    "                        \"n_distractor\": len(sampled_world[\"obj_map\"])-len(obj_map),\n",
    "                        \"full_relation_distractor\": full_relation_distractor,\n",
    "                        \"has_relation_distractor\": has_relation_distractor,\n",
    "                        \"has_attribute_distractor\": sampled_world[\"distractor_switch_map\"][\"attribute\"],\n",
    "                        \"has_isomorphism_distractor\": sampled_world[\"distractor_switch_map\"][\"isomorphism\"],\n",
    "                        \"has_random_distractor\": True if sampled_world[\"n_random_distractor\"] != -1 else False,\n",
    "                        \"n_random_distractor\": sampled_world[\"n_random_distractor\"] if sampled_world[\"n_random_distractor\"] != -1 else 0,\n",
    "                        \"relation_distractor_metadata\": sampled_world[\"relation_distractor_metadata\"],\n",
    "                        \"attribute_distractor_metadata\": sampled_world[\"attribute_distractor_metadata\"],\n",
    "                        \"isomorphism_distractor_metadata\": sampled_world[\"isomorphism_distractor_metadata\"],\n",
    "                        \"random_distractor_metadata\": sampled_world[\"random_distractor_metadata\"],\n",
    "                    })\n",
    "                    \n",
    "                    # Record distractor related info\n",
    "                    if task_struct[\"full_relation_distractor\"]:\n",
    "                        d_full_relation_count += 1\n",
    "                    if task_struct[\"has_relation_distractor\"]:\n",
    "                        d_relation_count += 1\n",
    "                    if task_struct[\"has_attribute_distractor\"]:\n",
    "                        d_attribute_count += 1\n",
    "                    if task_struct[\"has_isomorphism_distractor\"]:\n",
    "                        d_iso_count += 1\n",
    "                    if task_struct[\"n_random_distractor\"]:\n",
    "                        d_random_count += 1\n",
    "                    \n",
    "                    # Here, we decide which split we put the example into!\n",
    "                    split = args.mode\n",
    "                    created_examples_by_splits[split].append(task_struct)\n",
    "                    per_command_world_counts[command_struct_index] += 1\n",
    "                    break # break the retry loop!\n",
    "            if not at_least_success:\n",
    "                logger.info(f\"WARNING: the success rate for this command is close to 0.0%, skipping...\")\n",
    "                break # success rate for this comman is ~= 0.0%, let us directly skip\n",
    "        if save_at_interval and (command_struct_index+1)% save_interal == 0:\n",
    "            logger.info(f\"Saving data files and statistics to {args.output_dir} for checkpoints...\")\n",
    "            # Now, we need to save data into the folder\n",
    "            # along with possible statistics.\n",
    "            to_save_command_struct = []\n",
    "            per_command_count = []\n",
    "            for command_struct_index, count in per_command_world_counts.items():\n",
    "                per_command_count += [count]\n",
    "                if count >= 1:\n",
    "                    to_save_command_struct.append(sampled_command_struct_indexed[command_struct_index])\n",
    "            if save_command_stats:\n",
    "                _ = get_command_struct_statistics(\n",
    "                    to_save_command_struct, run_name=f\"ReaSCAN-{mode}\", date=args.date, \n",
    "                    split=mode,\n",
    "                    compositional_split=False,\n",
    "                    n_sample=-1,\n",
    "                    output_dir=args.output_dir,\n",
    "                    save_to_disk=True if args.output_dir != \"\" else False,\n",
    "                    wandb=wandb\n",
    "                )\n",
    "            \n",
    "            # wandb.log({\"per_command_world_count\": wandb.Histogram(per_command_count)})\n",
    "            \n",
    "            data_file_path = os.path.join(args.output_dir, f\"data-{args.mode}.txt\")\n",
    "            \n",
    "            if mode == \"demo\" or mode == \"all\" or mode == \"train\":\n",
    "                logger.info(f\"total example count={success_step}...\")\n",
    "                dataset_representation = {\n",
    "                    \"grid_size\": args.grid_size,\n",
    "                    \"type_grammar\": \"ReaSCAN-Grammer\",\n",
    "                    \"min_object_size\": 1,\n",
    "                    \"max_object_size\": 4,\n",
    "                    \"percentage_train\": split_percentage[\"train\"],\n",
    "                    \"examples\": created_examples_by_splits,\n",
    "                    \"intransitive_verbs\": intransitive_verbs,\n",
    "                    \"transitive_verbs\": transitive_verbs,\n",
    "                    \"adverbs\": adverbs,\n",
    "                    \"nouns\": nouns,\n",
    "                    \"color_adjectives\": color_adjectives,\n",
    "                    \"size_adjectives\": size_adjectives,\n",
    "                    \"relative_pronouns\": relative_pronouns,\n",
    "                    \"relation_clauses\": relation_clauses,\n",
    "                }\n",
    "                # dump to the disk.\n",
    "                with open(data_file_path, \"w\") as fd:\n",
    "                    json.dump(dataset_representation, fd, indent=4)\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "    # Last round of saving!\n",
    "    logger.info(f\"Saving FINAL data files and statistics to {args.output_dir}...\")\n",
    "    # Now, we need to save data into the folder\n",
    "    # along with possible statistics.\n",
    "    to_save_command_struct = []\n",
    "    per_command_count = []\n",
    "    for command_struct_index, count in per_command_world_counts.items():\n",
    "        per_command_count += [count]\n",
    "        if count >= 1:\n",
    "            to_save_command_struct.append(sampled_command_struct_indexed[command_struct_index])\n",
    "    if save_command_stats:\n",
    "        _ = get_command_struct_statistics(\n",
    "            to_save_command_struct, run_name=f\"ReaSCAN-{mode}\", date=args.date, \n",
    "            split=mode,\n",
    "            compositional_split=False,\n",
    "            n_sample=-1,\n",
    "            output_dir=args.output_dir,\n",
    "            save_to_disk=True if args.output_dir != \"\" else False,\n",
    "            wandb=wandb\n",
    "        )\n",
    "\n",
    "    # wandb.log({\"per_command_world_count\": wandb.Histogram(per_command_count)})\n",
    "\n",
    "    data_file_path = os.path.join(args.output_dir, f\"data-{args.mode}.txt\")\n",
    "\n",
    "    if mode == \"demo\" or mode == \"all\" or mode == \"train\":\n",
    "        logger.info(f\"total example count={success_step}...\")\n",
    "        dataset_representation = {\n",
    "            \"grid_size\": args.grid_size,\n",
    "            \"type_grammar\": \"ReaSCAN-Grammer\",\n",
    "            \"min_object_size\": 1,\n",
    "            \"max_object_size\": 4,\n",
    "            \"percentage_train\": split_percentage[\"train\"],\n",
    "            \"examples\": created_examples_by_splits,\n",
    "            \"intransitive_verbs\": intransitive_verbs,\n",
    "            \"transitive_verbs\": transitive_verbs,\n",
    "            \"adverbs\": adverbs,\n",
    "            \"nouns\": nouns,\n",
    "            \"color_adjectives\": color_adjectives,\n",
    "            \"size_adjectives\": size_adjectives,\n",
    "            \"relative_pronouns\": relative_pronouns,\n",
    "            \"relation_clauses\": relation_clauses,\n",
    "        }\n",
    "        # dump to the disk.\n",
    "        with open(data_file_path, \"w\") as fd:\n",
    "            json.dump(dataset_representation, fd, indent=4)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    logger.info(\"==FINISH==\")\n",
    "            \n",
    "    if args.is_tensorboard:\n",
    "        # end wandb\n",
    "        wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
